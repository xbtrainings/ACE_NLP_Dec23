{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WculWPI2Cwd8"
   },
   "source": [
    "# Step #3 : Supervised Learning (SL) of Reward Model (RM)\n",
    "\n",
    "## Task : from prompt, rank positive and negative responses\n",
    "\n",
    "## Use pre-trained SFT-LM model from Step #2 and froze it to use as backbone for training the reward\n",
    "\n",
    "### Xavier Bresson, xavier.bresson@gmail.com, https://twitter.com/xbresson\n",
    "\n",
    "### Number of data points for GPT-3, 175B parameters\n",
    "+ Step #1 : 300B tokens\n",
    "+ Step #2 : 10k-100k pairs (prompt, response)\n",
    "+ Step #3 : 100k-1M triples (prompt, positive response, negative response)\n",
    "+ Step #4 : 10k-100k prompts\n",
    "\n",
    "### Number of data points for this tutorial\n",
    "+ Step #1 : 3M tokens\n",
    "+ Step #2 : 10k pairs (prompt, response)\n",
    "+ Step #3 : 10k triples (prompt, positive response, negative response)\n",
    "+ Step #4 : 1k prompts\n",
    "\n",
    "### Objectives\n",
    "+ Supervised learning of reward\n",
    "+ Freeze a LM network to use as backbone for reward prediction\n",
    "+ Train with batch of pairs (prompt, positive response, negative) for fast training with GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25709,
     "status": "ok",
     "timestamp": 1699437635964,
     "user": {
      "displayName": "Xavier Bresson",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "nIgfWCW7CweA",
    "outputId": "c2a57a37-b308-45f6-8631-af495efa398f"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/ACE_NLP_Dec23_codes/codes/labs_vanillaLLMs'\n",
    "    print(path_to_file)\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7170,
     "status": "ok",
     "timestamp": 1699437643132,
     "user": {
      "displayName": "Xavier Bresson",
      "userId": "14103767471123103792"
     },
     "user_tz": -480
    },
    "id": "EPWi0LPuCweC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL) # remove warnings\n",
    "import os, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time stamp for save/load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_stamp: 23-11-24--13-06-33 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save time stamp\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "# check dataset folder exists \n",
    "data_dir = os.path.join('dataset')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# select a time stamp\n",
    "use_saved_time_stamp = False\n",
    "use_saved_time_stamp = True\n",
    "if use_saved_time_stamp:\n",
    "    time_stamp = '23-11-24--13-06-33' # trained on GPU on xxx\n",
    "\n",
    "print('time_stamp:', time_stamp, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dictionary of tokens from step #1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary: ['50', '56', '62', '68', '74', '80', '86', '<SEP>', '20', '24', '28', '32', '36', '14', '18', '22', '26', '30', '34', '38', '42', '46', '54', '58', '53', '61', '69', '77', '85', '93', '37', '45', '51', '66', '71', '76', '81', '91', '96', '0', '9', '27', '19', '23', '31', '35', '39', '43', '47', '2', '10', '33', '41', '49', '57', '65', '73', '89', '97', '88', '29', '44', '59', '64', '83', '92', '95', '98', '40', '15', '21', '25', '55', '60', '78', '87', '79', '84', '94', '48', '52', '90', '5', '8', '11', '17', '100', '99', '75', '63', '67', '70', '72', '3', '12', '82', '1', '7', '13', '16', '6', '4', 'generate', 'an', 'arithmetic', 'series', 'with', 'terms', 'starting', 'value', 'and', 'common', 'difference', 'Let', 'be', 'the', 'number', 'of', 'then', 'write', 'make', 'a', 'type', 'which', 'starts', 'at', 'elements', '<PAD>', '<EOS>'] \n",
      "\n",
      "num_tokens (unique): 129 \n",
      "\n",
      "token2index: {'50': 0, '56': 1, '62': 2, '68': 3, '74': 4, '80': 5, '86': 6, '<SEP>': 7, '20': 8, '24': 9, '28': 10, '32': 11, '36': 12, '14': 13, '18': 14, '22': 15, '26': 16, '30': 17, '34': 18, '38': 19, '42': 20, '46': 21, '54': 22, '58': 23, '53': 24, '61': 25, '69': 26, '77': 27, '85': 28, '93': 29, '37': 30, '45': 31, '51': 32, '66': 33, '71': 34, '76': 35, '81': 36, '91': 37, '96': 38, '0': 39, '9': 40, '27': 41, '19': 42, '23': 43, '31': 44, '35': 45, '39': 46, '43': 47, '47': 48, '2': 49, '10': 50, '33': 51, '41': 52, '49': 53, '57': 54, '65': 55, '73': 56, '89': 57, '97': 58, '88': 59, '29': 60, '44': 61, '59': 62, '64': 63, '83': 64, '92': 65, '95': 66, '98': 67, '40': 68, '15': 69, '21': 70, '25': 71, '55': 72, '60': 73, '78': 74, '87': 75, '79': 76, '84': 77, '94': 78, '48': 79, '52': 80, '90': 81, '5': 82, '8': 83, '11': 84, '17': 85, '100': 86, '99': 87, '75': 88, '63': 89, '67': 90, '70': 91, '72': 92, '3': 93, '12': 94, '82': 95, '1': 96, '7': 97, '13': 98, '16': 99, '6': 100, '4': 101, 'generate': 102, 'an': 103, 'arithmetic': 104, 'series': 105, 'with': 106, 'terms': 107, 'starting': 108, 'value': 109, 'and': 110, 'common': 111, 'difference': 112, 'Let': 113, 'be': 114, 'the': 115, 'number': 116, 'of': 117, 'then': 118, 'write': 119, 'make': 120, 'a': 121, 'type': 122, 'which': 123, 'starts': 124, 'at': 125, 'elements': 126, '<PAD>': 127, '<EOS>': 128} \n",
      "\n",
      "index2token: {0: '50', 1: '56', 2: '62', 3: '68', 4: '74', 5: '80', 6: '86', 7: '<SEP>', 8: '20', 9: '24', 10: '28', 11: '32', 12: '36', 13: '14', 14: '18', 15: '22', 16: '26', 17: '30', 18: '34', 19: '38', 20: '42', 21: '46', 22: '54', 23: '58', 24: '53', 25: '61', 26: '69', 27: '77', 28: '85', 29: '93', 30: '37', 31: '45', 32: '51', 33: '66', 34: '71', 35: '76', 36: '81', 37: '91', 38: '96', 39: '0', 40: '9', 41: '27', 42: '19', 43: '23', 44: '31', 45: '35', 46: '39', 47: '43', 48: '47', 49: '2', 50: '10', 51: '33', 52: '41', 53: '49', 54: '57', 55: '65', 56: '73', 57: '89', 58: '97', 59: '88', 60: '29', 61: '44', 62: '59', 63: '64', 64: '83', 65: '92', 66: '95', 67: '98', 68: '40', 69: '15', 70: '21', 71: '25', 72: '55', 73: '60', 74: '78', 75: '87', 76: '79', 77: '84', 78: '94', 79: '48', 80: '52', 81: '90', 82: '5', 83: '8', 84: '11', 85: '17', 86: '100', 87: '99', 88: '75', 89: '63', 90: '67', 91: '70', 92: '72', 93: '3', 94: '12', 95: '82', 96: '1', 97: '7', 98: '13', 99: '16', 100: '6', 101: '4', 102: 'generate', 103: 'an', 104: 'arithmetic', 105: 'series', 106: 'with', 107: 'terms', 108: 'starting', 109: 'value', 110: 'and', 111: 'common', 112: 'difference', 113: 'Let', 114: 'be', 115: 'the', 116: 'number', 117: 'of', 118: 'then', 119: 'write', 120: 'make', 121: 'a', 122: 'type', 123: 'which', 124: 'starts', 125: 'at', 126: 'elements', 127: '<PAD>', 128: '<EOS>'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_file_dictionary = 'dataset/step1_02_SSL_dictionary_23-11-23--12-26-17.pt'\n",
    "dictionary, num_tokens, token2index, index2token = torch.load(load_file_dictionary) # load dictionary of tokens\n",
    "print('dictionary:',dictionary,'\\n')\n",
    "print('num_tokens (unique):',num_tokens,'\\n')\n",
    "print('token2index:', token2index,'\\n')\n",
    "print('index2token:', index2token,'\\n')\n",
    "func_tokens2indices = lambda list_tokens: [token2index[token] for token in list_tokens] # ['Let', '5', 'be', 'the'] => [113, 46, 114, 115]\n",
    "func_indices2tokens = lambda list_ints: [index2token[integer] for integer in list_ints] # [113, 46, 114, 115] => ['Let', '5', 'be', 'the']\n",
    "func_str2tokens = lambda input_str: [token_str for token_str in input_str.split()]      # 'Let 5 be the' => ['Let', '5', 'be', 'the']\n",
    "func_tokens2str = lambda list_str: ' '.join(list_str)                                   # ['Let', '5', 'be', 'the'] => 'Let 5 be the'\n",
    "func_strs2pytorchs = lambda list_strs: torch.tensor([int(token_str) for token_str in list_strs])          # ['2', '4', '6', '8'] => tensor([2, 4, 6, 8]) \n",
    "func_pytorchs2strs = lambda list_pytorchs: ' '.join([str(pytorch) for pytorch in list_pytorchs.tolist()]) # tensor([2, 4, 6, 8]) => ['2', '4', '6', '8']\n",
    "func_ints2str = lambda list_ints: ' '.join([str(integer) for integer in list_ints]) # [8, 15, 22, 29] => '8 15 22 29'\n",
    "func_str2ints = lambda input_str: [int(string) for string in input_str.split()]     # '8 15 22 29' => [8, 15, 22, 29]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained SFT-LM network (step #2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A5000\n",
      "device: cuda \n",
      "\n",
      "Load pre-trained SFT-LM: \n",
      " checkpoint file: checkpoint/step2_checkpoint_SFT_LM_23-11-24--12-53-11.pkl\n",
      " epoch: 8, time: 586.115min, loss=0.0496\n",
      " num_tokens: 129, d: 384, context_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
      "\n",
      "num_tokens: 129, padding_int: 127, eos_int: 128\n",
      "\n",
      "num_net_parameters: 10761345 / 10.76 million\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # use same initial seed for reproducibility\n",
    "\n",
    "# compute number of network parameters\n",
    "def number_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    return nb_param\n",
    "\n",
    "# GPU training\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda\") # use GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('device:',device,'\\n')\n",
    "\n",
    "# token embedding layer : convert seq of integers to seq of vectors\n",
    "class token2vec(nn.Module):\n",
    "    def __init__(self, num_tokens, d):\n",
    "        super().__init__()\n",
    "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
    "    def forward(self, batch_int):\n",
    "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
    "        return batch_vec\n",
    "\n",
    "# multiple attention heads layer\n",
    "class multiple_head_attention(nn.Module):\n",
    "    def __init__(self, d, context_length, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads\n",
    "        assert d == d_head * num_heads # check divisiblity\n",
    "        self.MHA = nn.MultiheadAttention(d, num_heads, batch_first=True, dropout=dropout)\n",
    "        self.mask = torch.tril(torch.ones(context_length, context_length))==0 # mask to make attention to previous tokens only : { token(<=t) }, size=(context_length,context_length)\n",
    "                   # torch.tril(ones) = True in the up-right part, True means *no* attention allowed in pytorch implementation\n",
    "        self.context_length = context_length\n",
    "    def forward(self, H):\n",
    "        if H.size(1) == self.context_length: # training \n",
    "            attn_mask = self.mask\n",
    "        else: # when batch_length not= context_length, e.g. inference time / sequence generation \n",
    "            current_batch_length = H.size(1)\n",
    "            attn_mask = torch.tril(torch.ones(current_batch_length, current_batch_length))==0\n",
    "        H_heads = self.MHA(H, H, H, attn_mask=attn_mask.to(device))[0] # pytorch implementation, size=[batch_size, batch_length, d]\n",
    "        return H_heads\n",
    "\n",
    "# Transformer block layer\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d, context_length, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.MHA = multiple_head_attention(d, context_length, num_heads, dropout)\n",
    "        self.LN_MHA = nn.LayerNorm(d)\n",
    "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n",
    "        self.LN_MLP = nn.LayerNorm(d)\n",
    "    def forward(self, H):\n",
    "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, batch_length, d]\n",
    "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, batch_length, d]\n",
    "        return H\n",
    "\n",
    "# class of supervised fine-tuning LM network (step 2)\n",
    "class SFT_LM(nn.Module):\n",
    "    def __init__(self, num_tokens, d, context_length, num_heads, dropout, num_layers, padding_int, eos_int):\n",
    "        super().__init__()\n",
    "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
    "        self.seq_pos_encoding = torch.arange(context_length, device=device) # positional encoding = {0,1,2,...,context_length-1}\n",
    "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
    "        self.transformer_blocks = nn.ModuleList([ TransformerBlock(d, context_length, num_heads, dropout) for _ in range(num_layers) ]) # multiple transformer block layers\n",
    "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
    "        self.context_length = context_length\n",
    "        self.padding = padding_int\n",
    "        self.eos = eos_int \n",
    "    # Note : No forward function is needed\n",
    "        \n",
    "# load pre-trained SFT-LM network (step 2)\n",
    "checkpoint_file = \"checkpoint/step2_checkpoint_SFT_LM_23-11-24--12-53-11.pkl\"\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "epoch = checkpoint['epoch']\n",
    "tot_time = checkpoint['tot_time']\n",
    "loss = checkpoint['loss']\n",
    "print('Load pre-trained SFT-LM: \\n checkpoint file: {:s}\\n epoch: {:d}, time: {:.3f}min, loss={:.4f}'.format(checkpoint_file,epoch,tot_time,loss))\n",
    "net_parameters = SFT_LM_net_parameters = checkpoint['net_parameters']\n",
    "num_tokens = net_parameters['num_tokens']\n",
    "d = net_parameters['d']\n",
    "num_heads = net_parameters['num_heads']\n",
    "context_length = net_parameters['context_length']\n",
    "dropout = net_parameters['dropout']\n",
    "num_layers = net_parameters['num_layers']\n",
    "padding_int = net_parameters['padding_int']\n",
    "eos_int = net_parameters['eos_int']\n",
    "print(' num_tokens: %d, d: %d, context_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, context_length, num_heads, dropout, num_layers) )\n",
    "padding_int = torch.tensor([func_tokens2indices('<PAD>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
    "eos_int = torch.tensor([func_tokens2indices('<EOS>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
    "print('num_tokens: %d, padding_int: %d, eos_int: %d\\n' % (num_tokens, padding_int, eos_int))\n",
    "SFT_LMnet = SFT_LM(num_tokens, d, context_length, num_heads, dropout, num_layers, padding_int, eos_int)\n",
    "SFT_LMnet = SFT_LMnet.to(device)\n",
    "SFT_LMnet.load_state_dict(checkpoint['SFT_LMnet_dict']) # load pre-trained SFT-LM network (step 2)\n",
    "num_param = number_param(SFT_LMnet)\n",
    "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
    "del checkpoint\n",
    "\n",
    "# generate new sentence of any length\n",
    "def generate(LMnet, prompt, max_length_gen_seq):\n",
    "    #LMnet.train()\n",
    "    predicted_seq = torch.ones(1, max(prompt.size(0),LMnet.context_length)).long().to(device) * LMnet.padding # initiliaze with padding\n",
    "    predicted_seq[:, -prompt.size(0):] = prompt # fill batch_predicted_seq with prompt, right-aligned\n",
    "    for k in range(max_length_gen_seq):\n",
    "        context = predicted_seq[:,-LMnet.context_length:] # size=[batch_size, context_length\n",
    "        H = LMnet.token2vec(context) + LMnet.PE_embedding(LMnet.seq_pos_encoding[:context.size(1)]).unsqueeze(0) # size=[batch_size, context_length, d]\n",
    "        for transformer_block in LMnet.transformer_blocks: H = transformer_block(H) # size=(batch_size, context_length, d)\n",
    "        token_scores = H[:,-1,:] # extract last token to predict the next one, size=[batch_size, d]\n",
    "        token_scores = LMnet.token_prediction(token_scores) # compute scores, size=[batch_size, num_tokens]\n",
    "        token_probs = torch.softmax(token_scores, dim=1) # compute probs, size=[batch_size, num_tokens]\n",
    "        next_token = torch.multinomial(token_probs, num_samples=1) # sample next token, size=[batch_size, 1]\n",
    "        #next_token = torch.max(token_probs, dim=1).indices[0].view(1,1) # size=(1,1)\n",
    "        if next_token==LMnet.eos:\n",
    "            break\n",
    "        predicted_seq = torch.cat((predicted_seq, next_token), dim=1) # size=[batch_size, current_seq_len+1]\n",
    "    gen_seq = predicted_seq[0][max(prompt.size(0),LMnet.context_length):]\n",
    "    return gen_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training set of (prompt, positive response, negative response) with rewards\n",
    "\n",
    "### Use pre-trained SFT-LM network (step #2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_file: dataset/step3_01_SLRM_training_set_23-11-24--13-06-33.pt \n",
      "\n",
      "number of training data (prompt, positive response, negative response) : 500 \n",
      "\n",
      "training_set[0]: \n",
      "  pos_response : Let 5 be the number of terms 15 the starting number and 5 the common difference then write the arithmetic series 15 20 25 30 35, pos_reward : 7.00 \n",
      "  neg_response : Let 5 be the number of terms 15 the starting number and 5 the common difference then write the arithmetic series 15 23 25 30 35, neg_reward : 6.11  \n",
      "\n",
      "training_set[1]: \n",
      "  pos_response : Let 12 be the number of terms 12 the starting number and 2 the common difference then write the arithmetic series 12 14 16 18 20 22 24 26 28 30 32 34, pos_reward : 7.00 \n",
      "  neg_response : Let 12 be the number of terms 12 the starting number and 2 the common difference then write the arithmetic series 12 13 16 18 20 22 24 26 28 30 32 34, neg_reward : 6.45  \n",
      "\n",
      "training_set[2]: \n",
      "  pos_response : make a series of arithmetic type which starts at 79 with 12 elements and 2 common difference value 79 81 83 85 87 89 91 93 95 97 99, pos_reward : 7.00 \n",
      "  neg_response : make a series of arithmetic type which starts at 79 with 12 elements and 2 common difference value 79 81 83 85 87 89 91 93 95 91 99, neg_reward : 5.82  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate arithmetic series\n",
    "m = max_value = 100 # maximum value in the sequence\n",
    "def arithmetic_series(m, s, d, n):\n",
    "    seq = []\n",
    "    for i in range(n):\n",
    "        v = s + i * d\n",
    "        if v <= m:\n",
    "            seq.append(v)\n",
    "        else:\n",
    "            break\n",
    "    return seq\n",
    "\n",
    "# Generate training data:\n",
    "#  for each prompt, we collect one positive response and the associated reward\n",
    "#                          and one negative response and the associated reward\n",
    "#\n",
    "# The response is generated auto-regressively with the pre-trained LM of step 2\n",
    "#\n",
    "# Two responses are sampled and the positive response is the one with the largest reward value \n",
    "#\n",
    "# Reward is defined as r = r_min + ( r_max - r_min ) / (  1 + beta * || exact_response - generated_response || )\n",
    "#                          with r_min=1 (worst), r_max=7 (best)\n",
    "#\n",
    "# Training data structure:\n",
    "#  list_positive_responses = [ [prompt_1 + positive_response_1], [positive_reward_1] \n",
    "#                              [prompt_2 + positive_response_2], [positive_reward_2] \n",
    "#                                ...\n",
    "#                              [prompt_N + positive_response_N], [positive_reward_N] ]\n",
    "#\n",
    "#  list_negative_responses = [ [prompt_1 + negative_response_1], [negative_reward_1] \n",
    "#                              [prompt_2 + negative_response_2], [negative_reward_2] \n",
    "#                                ...\n",
    "#                              [prompt_N + negative_response_N], [negative_reward_N] ]\n",
    "#\n",
    "save_training_data = False\n",
    "#save_training_data = True\n",
    "if save_training_data:\n",
    "    \n",
    "    # collect \"human\" training set\n",
    "    list_positive_response = [] # list of prompts + positive responses\n",
    "    list_negative_response = [] # list of prompts + negative responses\n",
    "    num_training_data = 12 # debug\n",
    "    num_training_data = 500 # number of pairs of (prompt, positive response) and (prompt, negative response), e.g. GPU 10,000 training data\n",
    "    start = time.time()\n",
    "    num_data = 0\n",
    "    num_iterations = 0\n",
    "    start = time.time()\n",
    "    while num_data < num_training_data:\n",
    "\n",
    "        # parameters for arithmetic series\n",
    "        m = max_value # maximum value in the sequence\n",
    "        s = torch.randint(low=0, high=m, size=(1,)).item() # starting integer of the series\n",
    "        d = torch.randint(low=1, high=10, size=(1,)).item() # value of common difference\n",
    "        n = torch.randint(low=5, high=15, size=(1,)).item() # number of element in the series\n",
    "        #print('max_value: %d, start_value: %d, common_difference: %d, number_of_terms: %d' % (m,s,d,n))\n",
    "\n",
    "        # generate prompt : sample a prompt between 3 candidate prompts\n",
    "        prompt = {}\n",
    "        prompt[1] = 'generate an arithmetic series with ' + str(n) + ' terms starting with value ' + str(s) + ' and common difference ' + str(d)\n",
    "        prompt[2] = 'make a series of arithmetic type which starts at ' + str(s) + ' with ' + str(n) + ' elements and ' + str(d) + ' common difference value'\n",
    "        prompt[3] = 'Let ' + str(n) + ' be the number of terms ' + str(s) + ' the starting number and ' + str(d) + ' the common difference then write the arithmetic series'\n",
    "        random_int = torch.randint(low=1, high=3+1, size=(1,)).item() # random number in {1,2,3}\n",
    "        prompt_str = prompt[random_int]\n",
    "        #print('prompt         :',prompt_str)\n",
    "        prompt_ind_pytorch = torch.tensor(func_tokens2indices(func_str2tokens(prompt_str))) # convert str to pytorch indices\n",
    "        prompt_seq1_ind_pytorch = prompt_ind_pytorch; prompt_seq2_ind_pytorch = prompt_ind_pytorch # initializing prompt+response \n",
    "    \n",
    "        # exact response\n",
    "        exact_response_token_pytorch = torch.tensor(arithmetic_series(m,s,d,n))\n",
    "        #print('exact_response :',func_ints2str(exact_response_token_pytorch.tolist()))\n",
    "        \n",
    "        # generate two responses\n",
    "        gen_seq1_ind_pytorch = generate(SFT_LMnet, prompt_ind_pytorch, max_length_gen_seq=20) # sample one response\n",
    "        gen_seq2_ind_pytorch = generate(SFT_LMnet, prompt_ind_pytorch, max_length_gen_seq=20) # sample another response\n",
    "        \n",
    "        # remove non-integer tokens, i.e. words\n",
    "        list_int = [str(x) for x in torch.arange(max_value).tolist()] # list of all integers in string format, e.g. ['0', '1', '2', ... , '99']\n",
    "        gen_seq1_token = func_indices2tokens(gen_seq1_ind_pytorch.tolist()) # convert to tokens, e.g. ['90', '46', 'make', '71']\n",
    "        gen_seq1_token = [str(i) for i in gen_seq1_token if i in list_int] # remove non-integer tokens, e.g. ['90', '46', '71']\n",
    "        gen_seq1_ind_pytorch = torch.tensor(func_tokens2indices(gen_seq1_token)) # back to pytorch indices, e.g. tensor([45, 68, 3])\n",
    "        gen_seq2_token = func_indices2tokens(gen_seq2_ind_pytorch.tolist()) # convert to tokens\n",
    "        gen_seq2_token = [str(i) for i in gen_seq2_token if i in list_int] # remove non-integer tokens\n",
    "        gen_seq2_ind_pytorch = torch.tensor(func_tokens2indices(gen_seq2_token)) # back to pytorch indices\n",
    "        #print('seq1           :',func_tokens2str(gen_seq1_token))\n",
    "        #print('seq2           :',func_tokens2str(gen_seq2_token))\n",
    "        \n",
    "        if gen_seq1_ind_pytorch.size(0)>0 and gen_seq2_ind_pytorch.size(0)>0: # generated sequences have integer tokens, otherwise go to bext generation\n",
    "\n",
    "            # concatenate prompt + generated sequences\n",
    "            prompt_seq1_ind_pytorch = torch.cat( (prompt_seq1_ind_pytorch, gen_seq1_ind_pytorch) ) # concatenate prompt + seq1\n",
    "            prompt_seq2_ind_pytorch = torch.cat( (prompt_seq2_ind_pytorch, gen_seq2_ind_pytorch) ) # concatenate prompt + seq2\n",
    "\n",
    "            # compute rewards\n",
    "            max_size = max(exact_response_token_pytorch.size(0), gen_seq1_ind_pytorch.size(0), gen_seq2_ind_pytorch.size(0)) # e.g. 4\n",
    "            exact_response_token_pytorch = nn.functional.pad(exact_response_token_pytorch,(0, max_size-exact_response_token_pytorch.size(0)), 'constant', exact_response_token_pytorch[-1]) # padding to get same-size vectors, e.g. tensor([ 0,  8, 16, 24])\n",
    "            gen_seq1_token_pytorch = func_strs2pytorchs(gen_seq1_token)        \n",
    "            gen_seq1_token_pytorch = nn.functional.pad(gen_seq1_token_pytorch,(0, max_size-gen_seq1_token_pytorch.size(0)), 'constant', gen_seq1_token_pytorch[-1]) # padding to get same-size vectors, e.g. tensor([ 0,  8, 8, 8])\n",
    "            gen_seq2_token_pytorch = func_strs2pytorchs(gen_seq2_token)        \n",
    "            gen_seq2_token_pytorch = nn.functional.pad(gen_seq2_token_pytorch,(0, max_size-gen_seq2_token_pytorch.size(0)), 'constant', gen_seq2_token_pytorch[-1]) # padding to get same-size vectors, e.g. tensor([ 0,  7, 15, 15])\n",
    "            r_min = 1; r_max = 7\n",
    "            reward1 = r_min + (r_max - r_min) * ( 1 + 0.1*( (exact_response_token_pytorch - gen_seq1_token_pytorch).abs() ).float().sum().sqrt() )**(-1)\n",
    "            reward2 = r_min + (r_max - r_min) * ( 1 + 0.1*( (exact_response_token_pytorch - gen_seq2_token_pytorch).abs() ).float().sum().sqrt() )**(-1)\n",
    "            #print('reward #1      : %.2f' % reward1.item() ) \n",
    "            #print('reward #2      : %.2f' % reward2.item() )\n",
    "\n",
    "            # add samples to training dataset if reward1 not= reward2\n",
    "            if reward1 > reward2:\n",
    "                list_positive_response.append([prompt_seq1_ind_pytorch, reward1])\n",
    "                list_negative_response.append([prompt_seq2_ind_pytorch, reward2])\n",
    "                num_data += 1\n",
    "            elif reward1 < reward2:\n",
    "                list_positive_response.append([prompt_seq2_ind_pytorch, reward2])\n",
    "                list_negative_response.append([prompt_seq1_ind_pytorch, reward1])\n",
    "                num_data += 1\n",
    "\n",
    "            # print\n",
    "            if not num_iterations%500: # 2 (debug), 1000 (GPU) \n",
    "                print('num_iterations: %d, num_data: %d, time(min): %.3f' % (num_iterations, num_data, (time.time()-start)/60) )\n",
    "                print('prompt         :',prompt_str)\n",
    "                print('exact_response :',func_ints2str(exact_response_token_pytorch.tolist()))\n",
    "                print('seq1           :',func_tokens2str(gen_seq1_token))\n",
    "                print('seq2           :',func_tokens2str(gen_seq2_token))\n",
    "                print('reward #1      : %.2f' % reward1.item() ) \n",
    "                print('reward #2      : %.2f' % reward2.item() )\n",
    "            num_iterations += 1\n",
    "        \n",
    "    # print\n",
    "    print('\\nnumber of training data (prompt, positive response, negative response) :',len(list_positive_response),'\\n')\n",
    "    for idx, (positive, negative) in enumerate(zip(list_positive_response[:3],list_negative_response[:3])):\n",
    "        pos_response = func_tokens2str(func_indices2tokens(positive[0].tolist()))\n",
    "        neg_response = func_tokens2str(func_indices2tokens(negative[0].tolist()))\n",
    "        pos_reward = positive[1].item()\n",
    "        neg_reward = negative[1].item()\n",
    "        print('training_set[%d]: ' % idx )\n",
    "        print('  pos_response : %s, pos_reward : %.2f ' % (pos_response, pos_reward) )\n",
    "        print('  neg_response : %s, neg_reward : %.2f ' % (neg_response, neg_reward), '\\n' )\n",
    "    \n",
    "    # save training data\n",
    "    save_file = data_dir + '/step3_01_SLRM_training_set_' + time_stamp + '.pt'\n",
    "    print('save_file:', save_file, '\\n')\n",
    "    torch.save([list_positive_response, list_negative_response],save_file) # save list of positive and negative responses\n",
    "\n",
    "else:\n",
    "      \n",
    "    # load training data\n",
    "    load_file = data_dir + '/step3_01_SLRM_training_set_' + time_stamp + '.pt'\n",
    "    print('load_file:', load_file, '\\n')\n",
    "    list_positive_response, list_negative_response = torch.load(load_file) # load list of positive and negative responses\n",
    "\n",
    "    # print\n",
    "    print('number of training data (prompt, positive response, negative response) :',len(list_positive_response),'\\n')\n",
    "    for idx, (positive, negative) in enumerate(zip(list_positive_response[:3],list_negative_response[:3])):\n",
    "        pos_response = func_tokens2str(func_indices2tokens(positive[0].tolist()))\n",
    "        neg_response = func_tokens2str(func_indices2tokens(negative[0].tolist()))\n",
    "        pos_reward = positive[1].item()\n",
    "        neg_reward = negative[1].item()\n",
    "        print('training_set[%d]: ' % idx )\n",
    "        print('  pos_response : %s, pos_reward : %.2f ' % (pos_response, pos_reward) )\n",
    "        print('  neg_response : %s, neg_reward : %.2f ' % (neg_response, neg_reward), '\\n' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get batch of sampled indices for (positive response, negative response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_prompt_response: 500, batch_size: 3, num_batch: 166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# batching parameters\n",
    "num_prompt_response = len(list_positive_response) # number of prompt sequences\n",
    "batch_size = 3 # debug\n",
    "#batch_size = 100 # batch size, 500 GPU \n",
    "num_batch = num_prompt_response // batch_size # number of batches\n",
    "print('num_prompt_response: %d, batch_size: %d, num_batch: %d\\n' % (num_prompt_response,batch_size,num_batch))\n",
    "\n",
    "# sample batch of prompt+response\n",
    "def get_batch(batch_size, list_prompt_response_idx):\n",
    "    batch_idx = torch.randperm(list_prompt_response_idx.size(0))[:batch_size] # sample B number of batch indices\n",
    "    batch_idx = list_prompt_response_idx[batch_idx] # and extract from remaining list of batch indices\n",
    "    if list_prompt_response_idx.size(0) > batch_size:\n",
    "        new_list_prompt_response_idx = torch.stack([i for i in list_prompt_response_idx if i not in batch_idx]) # remove the sampled batch indices from the list of indices\n",
    "    else:\n",
    "        new_list_prompt_response_idx = torch.tensor([]) # last batch of epoch, i.e. return empty list\n",
    "    return batch_idx, new_list_prompt_response_idx\n",
    "\n",
    "# # one epoch (debug)\n",
    "# list_prompt_response_idx = torch.arange(num_prompt_response) # list of prompt+response indices\n",
    "# for i in range(num_batch):\n",
    "#     print('batch :',i)\n",
    "#     print('list_prompt_response_idx (before) :',list_prompt_response_idx, list_prompt_response_idx.size())\n",
    "#     batch_idx, list_prompt_response_idx = get_batch(batch_size, list_prompt_response_idx) # sample a batch of indices (prompt,response)\n",
    "#     print('batch_idx :', batch_idx, batch_idx.size())\n",
    "#     print('list_prompt_response_idx (after) :',list_prompt_response_idx, list_prompt_response_idx.size(),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train reward model with supervised learning \n",
    "\n",
    "## Use pre-trained LM model from step #2 with frozen layers as backbone \n",
    "\n",
    "## Dataset is composed of (positive response, negative response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A5000\n",
      "device: cuda \n",
      "\n",
      "Parameters of pre-trained SFT-LM network (step 2)\n",
      " num_tokens: 129, d: 384, context_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
      " num_tokens: 129, padding_int: 127, eos_int: 128\n",
      "\n",
      "num_net_parameters: 10910338 / 10.91 million\n",
      "\n",
      "checkpoint file : checkpoint/step3_checkpoint_SL_RM_23-11-24--13-06-33.pkl \n",
      "\n",
      "num_prompt_response: 500, batch_size: 100, num_batch: 5\n",
      "\n",
      "num_epochs:  1001 \n",
      "\n",
      "Epoch: 0, time(min): 0.003, lr= 0.000003, loss_epoch: 43.714\n",
      "pos response token : Let 14 be the number of terms 33 the starting number and 3 the common difference then write the arithmetic series 33 36 39 42 45 48 51 54 57 60 63 66 69 72\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : -0.19049903750419617\n",
      "neg response token : Let 14 be the number of terms 33 the starting number and 3 the common difference then write the arithmetic series 33 36 39 42 45 48 51 54 57 60 63 66 69 66\n",
      "neg reward label   : 5.819474697113037\n",
      "neg reward pred    : -0.29708772897720337 \n",
      "\n",
      "Epoch: 100, time(min): 0.305, lr= 0.000300, loss_epoch: 0.510\n",
      "pos response token : generate an arithmetic series with 9 terms starting with value 4 and common difference 1 4 3 6 7 8 9 10 11 12\n",
      "pos reward label   : 6.256604194641113\n",
      "pos reward pred    : 6.0859551429748535\n",
      "neg response token : generate an arithmetic series with 9 terms starting with value 4 and common difference 1 4 3 4 7 8 9 10 11 12\n",
      "neg reward label   : 6.0\n",
      "neg reward pred    : 6.145545482635498 \n",
      "\n",
      "Epoch: 200, time(min): 0.610, lr= 0.000300, loss_epoch: 0.391\n",
      "pos response token : make a series of arithmetic type which starts at 55 with 11 elements and 3 common difference value 55 58 61 64 67 70 73 76 79 82 85\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 6.491787433624268\n",
      "neg response token : make a series of arithmetic type which starts at 55 with 11 elements and 3 common difference value 55 60 61 64 67 70 73 76 79 82 85\n",
      "neg reward label   : 6.256604194641113\n",
      "neg reward pred    : 6.328037261962891 \n",
      "\n",
      "Epoch: 300, time(min): 0.914, lr= 0.000300, loss_epoch: 0.355\n",
      "pos response token : Let 12 be the number of terms 26 the starting number and 2 the common difference then write the arithmetic series 26 28 30 32 34 36 38 40 42 44 46 48\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 6.432677745819092\n",
      "neg response token : Let 12 be the number of terms 26 the starting number and 2 the common difference then write the arithmetic series 26 28 30 32 34 40 38 40 42 44 46 48\n",
      "neg reward label   : 6.0\n",
      "neg reward pred    : 6.3436713218688965 \n",
      "\n",
      "Epoch: 400, time(min): 1.219, lr= 0.000300, loss_epoch: 0.316\n",
      "pos response token : make a series of arithmetic type which starts at 15 with 12 elements and 2 common difference value 15 17 19 21 23 25 27 29 31 33 35 37\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 6.345550537109375\n",
      "neg response token : make a series of arithmetic type which starts at 15 with 12 elements and 2 common difference value 15 17 19 21 23 27 27 35 31 33 35 43\n",
      "neg reward label   : 5.36628532409668\n",
      "neg reward pred    : 6.166634559631348 \n",
      "\n",
      "Epoch: 500, time(min): 1.524, lr= 0.000300, loss_epoch: 0.277\n",
      "pos response token : generate an arithmetic series with 6 terms starting with value 23 and common difference 3 23 26 29 32 35 38\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 6.560057640075684\n",
      "neg response token : generate an arithmetic series with 6 terms starting with value 23 and common difference 3 23 26 29 26 35 38\n",
      "neg reward label   : 5.819474697113037\n",
      "neg reward pred    : 6.587183475494385 \n",
      "\n",
      "Epoch: 600, time(min): 1.830, lr= 0.000300, loss_epoch: 0.250\n",
      "pos response token : generate an arithmetic series with 13 terms starting with value 2 and common difference 5 2 7 12 17 22 27 32 37 42 47 52 57 62\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 6.5833353996276855\n",
      "neg response token : generate an arithmetic series with 13 terms starting with value 2 and common difference 5 2 7 12 17 22 27 35 37 36 47 46 51 56\n",
      "neg reward label   : 4.948368072509766\n",
      "neg reward pred    : 5.24292516708374 \n",
      "\n",
      "Epoch: 700, time(min): 2.135, lr= 0.000300, loss_epoch: 0.227\n",
      "pos response token : make a series of arithmetic type which starts at 33 with 13 elements and 4 common difference value 33 37 41 45 49 53 57 61 65 69 73 77 81\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 6.541468620300293\n",
      "neg response token : make a series of arithmetic type which starts at 33 with 13 elements and 4 common difference value 33 40 41 45 49 53 57 61 65 75 73 77 81\n",
      "neg reward label   : 5.615384578704834\n",
      "neg reward pred    : 6.145182132720947 \n",
      "\n",
      "Epoch: 800, time(min): 2.441, lr= 0.000300, loss_epoch: 0.206\n",
      "pos response token : make a series of arithmetic type which starts at 2 with 14 elements and 9 common difference value 2 11 20 29 38 47 56 65 74 83 92 97\n",
      "pos reward label   : 5.903535842895508\n",
      "pos reward pred    : 6.105541706085205\n",
      "neg response token : make a series of arithmetic type which starts at 2 with 14 elements and 9 common difference value 2 11 20 29 38 53 56 71 74 83 92\n",
      "neg reward label   : 5.456294059753418\n",
      "neg reward pred    : 5.872077941894531 \n",
      "\n",
      "Epoch: 900, time(min): 2.746, lr= 0.000300, loss_epoch: 0.181\n",
      "pos response token : Let 6 be the number of terms 4 the starting number and 2 the common difference then write the arithmetic series 4 6 8 10 12 14\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 6.3134894371032715\n",
      "neg response token : Let 6 be the number of terms 4 the starting number and 2 the common difference then write the arithmetic series 4 6 8 16 12 14\n",
      "neg reward label   : 5.819474697113037\n",
      "neg reward pred    : 6.415818691253662 \n",
      "\n",
      "Epoch: 1000, time(min): 3.051, lr= 0.000300, loss_epoch: 0.177\n",
      "pos response token : Let 14 be the number of terms 12 the starting number and 7 the common difference then write the arithmetic series 12 19 26 33 40 47 54 61 68 75 82 89 96\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 7.152866363525391\n",
      "neg response token : Let 14 be the number of terms 12 the starting number and 7 the common difference then write the arithmetic series 12 19 26 33 40 47 54 61 62 75 82 89 90 99\n",
      "neg reward label   : 5.324953079223633\n",
      "neg reward pred    : 5.481176376342773 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # use same initial seed for reproducibility\n",
    "\n",
    "# compute number of network parameters\n",
    "def number_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    return nb_param\n",
    "\n",
    "# GPU training\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda\") # use GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('device:',device,'\\n')\n",
    "\n",
    "# token embedding layer : convert seq of integers to seq of vectors\n",
    "class token2vec(nn.Module):\n",
    "    def __init__(self, num_tokens, d):\n",
    "        super().__init__()\n",
    "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
    "    def forward(self, batch_int):\n",
    "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
    "        return batch_vec\n",
    "\n",
    "# multiple attention heads layer\n",
    "class multiple_head_attention(nn.Module):\n",
    "    def __init__(self, d, context_length, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads\n",
    "        assert d == d_head * num_heads # check divisiblity\n",
    "        self.MHA = nn.MultiheadAttention(d, num_heads, batch_first=True, dropout=dropout)\n",
    "        self.mask = torch.tril(torch.ones(context_length, context_length))==0 # mask to make attention to previous tokens only : { token(<=t) }, size=(context_length,context_length)\n",
    "                   # torch.tril(ones) = True in the up-right part, True means *no* attention allowed in pytorch implementation\n",
    "        self.context_length = context_length\n",
    "    def forward(self, H):\n",
    "        if H.size(1) == self.context_length: # training <==\n",
    "            attn_mask = self.mask\n",
    "        else: # when batch_length not= context_length, e.g. inference time / sequence generation <==\n",
    "            current_batch_length = H.size(1)\n",
    "            attn_mask = torch.tril(torch.ones(current_batch_length, current_batch_length))==0\n",
    "        H_heads = self.MHA(H, H, H, attn_mask=attn_mask.to(device))[0] # pytorch implementation, size=[batch_size, batch_length, d]\n",
    "        return H_heads\n",
    "\n",
    "# Transformer block layer\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d, context_length, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.MHA = multiple_head_attention(d, context_length, num_heads, dropout)\n",
    "        self.LN_MHA = nn.LayerNorm(d)\n",
    "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n",
    "        self.LN_MLP = nn.LayerNorm(d)\n",
    "    def forward(self, H):\n",
    "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, batch_length, d]\n",
    "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, batch_length, d]\n",
    "        return H\n",
    "    \n",
    "# Predict positive reward given context = { prompt + positive response }\n",
    "#         negative reward given context = { prompt + negative response }\n",
    "#\n",
    "# Prediction is done into 2 stages :\n",
    "#         1. compute self-attention between last token of the sequence and the context using pre-trained SFT-LM (step 2)\n",
    "#         2. apply small MLP to predict scalar reward\n",
    "#\n",
    "# Example of reward prediction for a BATCH of sequences = (prompt + positive/negative response) => GPU\n",
    "#\n",
    "# Prepare a batch of sampled (prompt+response)\n",
    "#  Let the token P = <Padding> \n",
    "#\n",
    "#                    context_size = 7 (all prompt+response have the same context length with padding if needed) => GPU\n",
    "#                 ---------------------\n",
    "# batch_seq    = [ P, P, 1, 2, 3, 4, 5 ]  // prompt = [1, 2, 3] + positive response = [4, 5]\n",
    "#              = [ 1, 2, 3, 4, 7, 8, 1 ]  // prompt = [1, 2, 3] + negative response = [4, 7, 8, 1]\n",
    "#                         ...\n",
    "#              = [ P, P, 5, 6, 7, 8, 9 ]  // prompt = [5, 6] + positive response = [7, 8, 9]\n",
    "#              = [ P, P, P, 5, 6, 3, 4 ]  // prompt = [5, 6] + negative response = [3, 4]\n",
    "#                        -------------\n",
    "#                                    | <= compute self-attention between last token = 4 and context = [5, 6, 3]\n",
    "#                                    | <= then apply MLP to predict reward \n",
    "# batch_reward_scores            = [ 5.3 ] // predicted positive reward for [1, 2, 3, 4, 5]\n",
    "#                                = [ 2.9 ] // predicted negative reward for [1, 2, 3, 4, 7, 8, 1]\n",
    "#                                    ...\n",
    "#                                = [ 6.2 ] // predicted positive reward for [5, 6, 7, 8, 9]\n",
    "#                                = [ 1.8 ] // predicted negative reward for [5, 6, 3, 4]\n",
    "#\n",
    "# Supervised learning network for reward (step 3)\n",
    "class SL_RM(nn.Module):\n",
    "    def __init__(self, SFT_LM, d, context_length, padding_int, eos_int):\n",
    "        super().__init__()\n",
    "        self.SFT_LM = SFT_LM # token embedding layer\n",
    "        self.reward_prediction = nn.Sequential(nn.LayerNorm(d), nn.Linear(d,d), nn.ReLU(), nn.Linear(d,1)) # reward prediction layer \n",
    "        self.context_length = context_length\n",
    "        self.padding = padding_int \n",
    "        self.eos = eos_int \n",
    "    def forward(self, batch_idx, list_positive_response, list_negative_response): # batch_idx.size=[batch_size], len(list_prompt,list_response) =[num_prompt_response]\n",
    "        pos_responses = [list_positive_response[idx][0] for idx in batch_idx] # sample list of pos_responses, len(pos_responses)=num_prompt_response\n",
    "        len_pos_response = max([len(response) for response in pos_responses]) # compute max of pos_response lengths\n",
    "        neg_responses = [list_negative_response[idx][0] for idx in batch_idx] # sample list of neg_responses, len(neg_responses)=num_prompt_response\n",
    "        len_neg_response = max([len(response) for response in neg_responses]) # compute max of neg_response lengths\n",
    "        len_response = max(len_pos_response, len_neg_response) # compute max of pos_response and neg_response lengths\n",
    "        batch_size = batch_idx.size(0)\n",
    "        batch_seq = torch.ones(2* batch_size, max(len_response,self.context_length)).long().to(device) * self.padding # context, initialize with padding, size=[2* batch_size, context_length]\n",
    "        for idx in range(batch_size): batch_seq[2*idx, -pos_responses[idx].size(0):] = pos_responses[idx] # fill context with pos_responses, right-aligned\n",
    "        for idx in range(batch_size): batch_seq[2*idx+1, -neg_responses[idx].size(0):] = neg_responses[idx] # fill context with neg_responses, right-aligned\n",
    "        H = self.SFT_LM.token2vec(batch_seq) + self.SFT_LM.PE_embedding(self.SFT_LM.seq_pos_encoding[:batch_seq.size(1)]).unsqueeze(0) # size=[2* batch_size, context_length, d]\n",
    "        for transformer_block in self.SFT_LM.transformer_blocks: H = transformer_block(H) # size=[2* batch_size, context_length, d)\n",
    "        token_scores = H[:,-1,:] # extract last token scores to predict rewards, size=[2* batch_size, d]\n",
    "        reward_scores = self.reward_prediction(token_scores) # compute reward scores, size=[2* batch_size, 1]\n",
    "        return reward_scores\n",
    "        \n",
    "# use parameters of pre-trained SFT-LM network (step 2) for SL_RM network\n",
    "print('Parameters of pre-trained SFT-LM network (step 2)')\n",
    "num_tokens = net_parameters['num_tokens']\n",
    "d = net_parameters['d']\n",
    "num_heads = net_parameters['num_heads']\n",
    "context_length = net_parameters['context_length']\n",
    "dropout = net_parameters['dropout']\n",
    "num_layers = net_parameters['num_layers']\n",
    "padding_int = net_parameters['padding_int']\n",
    "eos_int = net_parameters['eos_int']\n",
    "print(' num_tokens: %d, d: %d, context_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d' % (num_tokens, d, context_length, num_heads, dropout, num_layers) )\n",
    "padding_int = torch.tensor([func_tokens2indices('<PAD>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
    "eos_int = torch.tensor([func_tokens2indices('<EOS>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
    "print(' num_tokens: %d, padding_int: %d, eos_int: %d\\n' % (num_tokens, padding_int, eos_int))\n",
    "\n",
    "# SL_RM network\n",
    "SL_RMnet = SL_RM(SFT_LMnet, d, context_length, padding_int, eos_int)\n",
    "SL_RMnet = SL_RMnet.to(device)\n",
    "num_param = number_param(SL_RMnet)\n",
    "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
    "\n",
    "# freeze SFT-LM network (step 2) during training\n",
    "for name, param in SL_RMnet.named_parameters():\n",
    "    if param.requires_grad and 'SFT_LM' in name:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(SL_RMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
    "warmup = 500 # 50(debug), 500(GPU), number of batches used for warmup \n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min(t/warmup, 1.0) ) # warmup learning rate scheduler, good for LM (softmax)\n",
    "\n",
    "# save checkpoint\n",
    "net_parameters = SFT_LM_net_parameters  \n",
    "checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print('checkpoint file :', checkpoint_dir + '/step3_checkpoint_SL_RM_' + time_stamp + '.pkl', '\\n')\n",
    "\n",
    "# batching parameters\n",
    "num_prompt_response = len(list_positive_response) # number of prompt+response sequences\n",
    "batch_size = 3 # debug\n",
    "batch_size = 100 # batch size, 100 GPU <==\n",
    "num_batch = num_prompt_response // batch_size # number of batches\n",
    "print('num_prompt_response: %d, batch_size: %d, num_batch: %d\\n' % (num_prompt_response, batch_size, num_batch))\n",
    "\n",
    "# Understanding the loss\n",
    "#\n",
    "# 1. loss_rewards = MSE( predicted_reward, label_reward )\n",
    "#\n",
    "# reward_scores                  = [ 5.3 ] // predicted positive reward for seq_1 <= index 2*i   for pos, i = 0, 1, ..., num_prompts-1\n",
    "#   = predicted_reward           = [ 2.9 ] // predicted negative reward for seq_1 <= index 2*i+1 for neg, i = 0, 1, ..., num_prompts-1\n",
    "#                                    ...\n",
    "#                                = [ 6.2 ] // predicted Positive reward for seq_B\n",
    "#                                = [ 1.8 ] // predicted Negative reward for seq_B\n",
    "#\n",
    "# 2. loss_rank = - log( sigmoid( positive_reward - negative_reward ) )\n",
    "#\n",
    "# 3. total_loss = loss_rewards + cst * loss_rank\n",
    "#\n",
    "# Train network to predict reward from (pos_response, neg_response)\n",
    "num_epochs = 1001 # 1001(debug), 1001(GPU), number of epochs \n",
    "print('num_epochs: ',num_epochs,'\\n')\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs): # number of epochs\n",
    "    list_prompt_response_idx = torch.arange(num_prompt_response).to(device) # initialize the list of (pos_response, neg_response) indices\n",
    "    running_loss = 0.0 # tracking total loss value\n",
    "    for k in range(num_batch): # number of batches in one epoch\n",
    "        batch_idx, list_prompt_response_idx = get_batch(batch_size, list_prompt_response_idx) # sample a batch of indices (pos_response, neg_response)\n",
    "        reward_scores = SL_RMnet(batch_idx.to(device), list_positive_response, list_negative_response) # predict rewards, size=[2*batch_size, 1]\n",
    "        reward_labels = [ [list_positive_response[idx][1],list_negative_response[idx][1]] for idx in batch_idx ]\n",
    "        reward_labels = torch.tensor(reward_labels).view(2*batch_size,1).to(device) # size=[2*batch_size, 1]\n",
    "        diff_rewards = reward_scores[0:2*batch_size:2,:] - reward_scores[1:2*batch_size+1:2,:] # difference of rewards, sise=[batch_size, 1]\n",
    "        loss_rank = - torch.log(torch.sigmoid(diff_rewards)).mean() # rank loss\n",
    "        loss_mse = nn.MSELoss()(reward_scores, reward_labels) # regression loss for rewards\n",
    "        loss = 1.0*loss_mse + 0.1* loss_rank\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    loss_epoch = running_loss / num_batch\n",
    "    if not epoch%100: # 1(debug), 100(GPU)\n",
    "        print('Epoch: %d, time(min): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, (time.time()-start)/60, optimizer.param_groups[0]['lr'], loss_epoch) )\n",
    "        # print rewards\n",
    "        idx_prompt = 0\n",
    "        print('pos response token :',func_tokens2str(func_indices2tokens(list_positive_response[batch_idx[idx_prompt]][0].tolist())))\n",
    "        print('pos reward label   :',reward_labels[2*idx_prompt,:].squeeze().item())\n",
    "        print('pos reward pred    :',reward_scores[2*idx_prompt,:].squeeze().item())\n",
    "        print('neg response token :',func_tokens2str(func_indices2tokens(list_negative_response[batch_idx[idx_prompt]][0].tolist())))\n",
    "        print('neg reward label   :',reward_labels[2*idx_prompt+1,:].squeeze().item())\n",
    "        print('neg reward pred    :',reward_scores[2*idx_prompt+1,:].squeeze().item(),'\\n')\n",
    "        # save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'tot_time': time.time()-start,\n",
    "            'loss': loss_epoch,\n",
    "            'net_parameters': net_parameters,\n",
    "            'SL_RMnet_dict': SL_RMnet.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            }, '{}.pkl'.format(checkpoint_dir + \"/step3_checkpoint_SL_RM_\" + time_stamp ))\n",
    "        # Stopping condition\n",
    "        if loss_epoch < 0.01: \n",
    "            print(\"\\n loss value is small -- training stopped\\n\")\n",
    "            break\n",
    "\n",
    "# GPU training time : Epoch: 1000, time(min): 3.051, lr= 0.000300, loss_epoch: 0.177\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained SL-RM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained SFT-LM: \n",
      " checkpoint file: checkpoint/step3_checkpoint_SL_RM_23-11-24--13-06-33.pkl\n",
      " epoch: 1000, time: 183.084min, loss=0.1769\n",
      " num_tokens: 129, d: 384, context_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
      "\n",
      "num_tokens: 129, padding_int: 127, eos_int: 128\n",
      "\n",
      "num_net_parameters: 10910338 / 10.91 million\n",
      "\n",
      "pos response token : generate an arithmetic series with 10 terms starting with value 46 and common difference 3 46 49 52 55 58 61 64 67 70 73\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 7.064010143280029\n",
      "neg response token : generate an arithmetic series with 10 terms starting with value 46 and common difference 3 46 49 52 55 58 61 64 67 64 73\n",
      "neg reward label   : 5.819474697113037\n",
      "neg reward pred    : 5.924700736999512 \n",
      "\n",
      "pos response token : make a series of arithmetic type which starts at 15 with 12 elements and 2 common difference value 15 17 19 21 23 25 27 29 31 33 35 37\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 6.833649158477783\n",
      "neg response token : make a series of arithmetic type which starts at 15 with 12 elements and 2 common difference value 15 17 19 21 23 27 27 35 31 33 35 43\n",
      "neg reward label   : 5.36628532409668\n",
      "neg reward pred    : 5.573583602905273 \n",
      "\n",
      "pos response token : Let 13 be the number of terms 71 the starting number and 8 the common difference then write the arithmetic series 71 79 87 95\n",
      "pos reward label   : 7.0\n",
      "pos reward pred    : 7.190972328186035\n",
      "neg response token : Let 13 be the number of terms 71 the starting number and 8 the common difference then write the arithmetic series 71 79 85 95 99\n",
      "neg reward label   : 5.819474697113037\n",
      "neg reward pred    : 5.540439605712891 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pre-trained SL-RM network\n",
    "checkpoint_file = 'checkpoint_file/step3_checkpoint_SL_RM_23-11-24--13-06-33.pkl'\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "epoch = checkpoint['epoch']\n",
    "tot_time = checkpoint['tot_time']\n",
    "loss = checkpoint['loss']\n",
    "print('Load pre-trained SFT-LM: \\n checkpoint file: {:s}\\n epoch: {:d}, time: {:.3f}min, loss={:.4f}'.format(checkpoint_file,epoch,tot_time,loss))\n",
    "net_parameters = checkpoint['net_parameters']\n",
    "num_tokens = net_parameters['num_tokens']\n",
    "d = net_parameters['d']\n",
    "num_heads = net_parameters['num_heads']\n",
    "context_length = net_parameters['context_length']\n",
    "dropout = net_parameters['dropout']\n",
    "num_layers = net_parameters['num_layers']\n",
    "padding_int = net_parameters['padding_int']\n",
    "eos_int = net_parameters['eos_int']\n",
    "print(' num_tokens: %d, d: %d, context_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, context_length, num_heads, dropout, num_layers) )\n",
    "padding_int = torch.tensor([func_tokens2indices('<PAD>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
    "eos_int = torch.tensor([func_tokens2indices('<EOS>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
    "print('num_tokens: %d, padding_int: %d, eos_int: %d\\n' % (num_tokens, padding_int, eos_int))\n",
    "SL_RMnet = SL_RM(SFT_LMnet, d, context_length, padding_int, eos_int)\n",
    "SL_RMnet = SL_RMnet.to(device)\n",
    "SL_RMnet.load_state_dict(checkpoint['SL_RMnet_dict']) # load pre-trained SL-RM network from step #3\n",
    "num_param = number_param(SL_RMnet)\n",
    "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
    "del checkpoint\n",
    "    \n",
    "# print\n",
    "batch_idx = torch.randperm(len(list_positive_response))[:3] # select 3 indices \n",
    "reward_scores = SL_RMnet(batch_idx.to(device), list_positive_response, list_negative_response) # predict rewards, size=[2*3, 1]\n",
    "reward_labels = [ [list_positive_response[idx][1],list_negative_response[idx][1]] for idx in batch_idx ]\n",
    "reward_labels = torch.tensor(reward_labels).view(2*batch_idx.size(0),1) # size=[2*3, 1]\n",
    "for idx in range(batch_idx.size(0)): \n",
    "    print('pos response token :',func_tokens2str(func_indices2tokens(list_positive_response[batch_idx[idx]][0].tolist())))\n",
    "    print('pos reward label   :',reward_labels[2*idx,:].squeeze().item())\n",
    "    print('pos reward pred    :',reward_scores[2*idx,:].squeeze().item())\n",
    "    print('neg response token :',func_tokens2str(func_indices2tokens(list_negative_response[batch_idx[idx]][0].tolist())))\n",
    "    print('neg reward label   :',reward_labels[2*idx+1,:].squeeze().item())\n",
    "    print('neg reward pred    :',reward_scores[2*idx+1,:].squeeze().item(),'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
