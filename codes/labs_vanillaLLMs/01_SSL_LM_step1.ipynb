{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WculWPI2Cwd8"
      },
      "source": [
        "# Step #1 : Self-Supervised Learning (SSL) of Language Model (LM)\n",
        "\n",
        "## Task : from context, predict next word\n",
        "\n",
        "### Xavier Bresson, xavier.bresson@gmail.com, https://twitter.com/xbresson\n",
        "\n",
        "### Number of data points for GPT-3, 175B parameters\n",
        "+ Step #1 : 300B tokens\n",
        "+ Step #2 : 10k-100k pairs (prompt, response)\n",
        "+ Step #3 : 100k-1M triples (prompt, positive response, negative response)\n",
        "+ Step #4 : 10k-100k prompts\n",
        "\n",
        "### Number of data points for this tutorial\n",
        "+ Step #1 : 3M tokens\n",
        "+ Step #2 : 10k pairs (prompt, response)\n",
        "+ Step #3 : 10k triples (prompt, positive response, negative response)\n",
        "+ Step #4 : 1k prompts\n",
        "\n",
        "### Objectives\n",
        "+ Step-by-step approach to self-supervised learning of LM\n",
        "+ Implementation of word prediction with { previous word, bag-of-words, attention mechanism }\n",
        "+ Implement Transformer architecture with single head, multiple heads, PE, RC, LN, MLP, and dropout\n",
        "+ Train with batch of sequences for fast training with GPU\n",
        "+ Save the pre-trained LM network for Step #2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIgfWCW7CweA",
        "outputId": "449b3855-3a46-46af-9e93-46a01637e261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/ACE_NLP_Dec23_codes/codes/labs_vanillaLLMs\n",
            "/content/gdrive/My Drive/ACE_NLP_Dec23_codes/codes/labs_vanillaLLMs\n"
          ]
        }
      ],
      "source": [
        "# For Google Colaboratory\n",
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    # mount google drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    path_to_file = '/content/gdrive/My Drive/ACE_NLP_Dec23_codes/codes/labs_vanillaLLMs'\n",
        "    print(path_to_file)\n",
        "    # move to Google Drive directory\n",
        "    os.chdir(path_to_file)\n",
        "    !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EPWi0LPuCweC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c4690f-d006-480a-dc24-f48b6d028a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ],
      "source": [
        "# Libraries\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.CRITICAL) # remove warnings\n",
        "import os, datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM99qcVnYnVA"
      },
      "source": [
        "## Time stamp for save/load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhjqFG8DYnVA",
        "outputId": "59e11744-fa21-4a6d-ecc9-3c811bf17147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time_stamp: 23-12-05--01-02-01 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# save time stamp\n",
        "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
        "\n",
        "# check dataset folder exists\n",
        "data_dir = os.path.join('dataset')\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# select a time stamp\n",
        "use_saved_time_stamp = False\n",
        "#use_saved_time_stamp = True\n",
        "if use_saved_time_stamp:\n",
        "    time_stamp = '23-11-23--12-26-17' # trained on GPU on '23-11-23--12-26-17'\n",
        "\n",
        "print('time_stamp:', time_stamp, '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdIf3zFaBfis"
      },
      "source": [
        "## Generate training sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CeC2DXrzm5Jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50a81f3-804d-4eae-c066-ab7d507fae05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_value: 100, start_value: 0, common_difference: 3, number_of_terms: 14\n",
            "an arithmetic series: [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39]\n",
            "len(seq) data: 3000000, time(sec): 7.685\n",
            "number of tokens in the sequence : 3000000\n",
            "print first 50 tokens : ['18', '23', '28', '33', '38', '43', '<SEP>', '67', '76', '85', '94', '<SEP>', '61', '68', '75', '82', '89', '96', '<SEP>', '47', '53', '59', '65', '71', '77', '83', '89', '95', '<SEP>', '50', '53', '56', '59', '62', '65', '68', '71', '74', '77', '80', '83', '86', '89', '<SEP>', '86', '87', '88', '89', '90', '<SEP>'] \n",
            "\n",
            "save_file: dataset/step1_01_SSL_training_set_token_23-12-05--01-02-01.pt \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# generate arithmetic series\n",
        "m = max_value = 100 # maximum value in the sequence\n",
        "def arithmetic_series(m, s, d, n):\n",
        "    seq = []\n",
        "    for i in range(n):\n",
        "        v = s + i * d\n",
        "        if v <= m:\n",
        "            seq.append(v)\n",
        "        else:\n",
        "            break\n",
        "    return seq\n",
        "\n",
        "# generate training data, i.e. a long sequence of tokens as\n",
        "#  seq = [ 2, 4, 6, <SEP>, 14, 17, 20, 22, <SEP>, 6, 8, ... ]\n",
        "save_training_data = False\n",
        "save_training_data = True\n",
        "if save_training_data:\n",
        "\n",
        "    # parameters for arithmetic series\n",
        "    m = max_value # maximum value in the sequence\n",
        "    s = torch.randint(low=0, high=m, size=(1,)).item() # starting integer of the series\n",
        "    d = torch.randint(low=1, high=10, size=(1,)).item() # value of common difference\n",
        "    n = torch.randint(low=5, high=15, size=(1,)).item() # number of element in the series\n",
        "    print('max_value: %d, start_value: %d, common_difference: %d, number_of_terms: %d' % (m,s,d,n))\n",
        "    seq = arithmetic_series(m,s,d,n)\n",
        "    print('an arithmetic series:',seq)\n",
        "\n",
        "    # generate and save a sequence of arithmetic series separated with token <SEP>\n",
        "    len_dataset = 100 # debug, e.g. 100\n",
        "    len_dataset = 3000000 # length of the sequence of arithmetic series, e.g. 3M\n",
        "    seq = []\n",
        "    separator_token = '<SEP>' # separator token between series\n",
        "    start = time.time()\n",
        "    while len(seq)<=len_dataset:\n",
        "        s = torch.randint(low=0, high=m, size=(1,)).item() # starting integer of the series\n",
        "        d = torch.randint(low=1, high=10, size=(1,)).item() # value of common difference\n",
        "        n = torch.randint(low=5, high=15, size=(1,)).item() # number of element in the series\n",
        "        series = arithmetic_series(m,s,d,n) # generate arithmetic series\n",
        "        series_token = [str(i) for i in series] # convert seq of integers into seq of tokens w/ string type\n",
        "        series_token.append(separator_token) # append separator token\n",
        "        seq.extend(series_token) # append one generated series to the sequence\n",
        "    seq_tokens = seq[:len_dataset] # truncate the sequence to \"len_dataset\" number of tokens\n",
        "    print('len(seq) data: %d, time(sec): %.3f' % (len(seq_tokens), time.time()-start) )\n",
        "\n",
        "    # print\n",
        "    print('number of tokens in the sequence :',len(seq_tokens))\n",
        "    print('print first 50 tokens :',seq_tokens[:50],'\\n')\n",
        "\n",
        "    # save training data\n",
        "    save_file = data_dir + '/step1_01_SSL_training_set_token_' + time_stamp + '.pt'\n",
        "    print('save_file:', save_file, '\\n')\n",
        "    torch.save([seq_tokens],save_file) # save the sequence\n",
        "\n",
        "else:\n",
        "\n",
        "    # load data\n",
        "    load_file = data_dir + '/step1_01_SSL_training_set_token_' + time_stamp + '.pt'\n",
        "    print('load_file:', load_file, '\\n')\n",
        "    seq_tokens = torch.load(load_file)[0]\n",
        "    print('number of tokens in the sequence :',len(seq_tokens))\n",
        "    print('print first 50 tokens :',seq_tokens[:50])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRGLjLBtBfit"
      },
      "source": [
        "## Get dictionary of tokens and convert sequence of tokens to integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY8v3bw7m5Jj",
        "outputId": "1ddc3abc-6127-4e2a-ffc2-35887d15f741",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_file: dataset/step1_01_SSL_training_set_token_23-12-05--01-02-01.pt \n",
            "\n",
            "number of tokens in the sequence : 3000000 \n",
            "\n",
            "dictionary: ['18', '23', '28', '33', '38', '43', '<SEP>', '67', '76', '85', '94', '61', '68', '75', '82', '89', '96', '47', '53', '59', '65', '71', '77', '83', '95', '50', '56', '62', '74', '80', '86', '87', '88', '90', '93', '100', '10', '12', '14', '16', '20', '22', '24', '21', '26', '31', '36', '41', '46', '63', '69', '81', '99', '70', '72', '73', '78', '1', '8', '15', '29', '57', '5', '7', '9', '11', '13', '6', '55', '30', '39', '48', '66', '84', '34', '37', '40', '49', '52', '58', '64', '91', '25', '79', '97', '54', '27', '32', '35', '17', '45', '42', '92', '19', '98', '4', '3', '51', '44', '60', '2', '0'] \n",
            "\n",
            "num_tokens (unique): 102 \n",
            "\n",
            "updated dictionary: ['18', '23', '28', '33', '38', '43', '<SEP>', '67', '76', '85', '94', '61', '68', '75', '82', '89', '96', '47', '53', '59', '65', '71', '77', '83', '95', '50', '56', '62', '74', '80', '86', '87', '88', '90', '93', '100', '10', '12', '14', '16', '20', '22', '24', '21', '26', '31', '36', '41', '46', '63', '69', '81', '99', '70', '72', '73', '78', '1', '8', '15', '29', '57', '5', '7', '9', '11', '13', '6', '55', '30', '39', '48', '66', '84', '34', '37', '40', '49', '52', '58', '64', '91', '25', '79', '97', '54', '27', '32', '35', '17', '45', '42', '92', '19', '98', '4', '3', '51', '44', '60', '2', '0', 'generate', 'an', 'arithmetic', 'series', 'with', 'terms', 'starting', 'value', 'and', 'common', 'difference', 'Let', 'be', 'the', 'number', 'of', 'then', 'write', 'make', 'a', 'type', 'which', 'starts', 'at', 'elements', '<PAD>', '<EOS>'] \n",
            "\n",
            "num_tokens (unique): 129 \n",
            "\n",
            "token2index: {'18': 0, '23': 1, '28': 2, '33': 3, '38': 4, '43': 5, '<SEP>': 6, '67': 7, '76': 8, '85': 9, '94': 10, '61': 11, '68': 12, '75': 13, '82': 14, '89': 15, '96': 16, '47': 17, '53': 18, '59': 19, '65': 20, '71': 21, '77': 22, '83': 23, '95': 24, '50': 25, '56': 26, '62': 27, '74': 28, '80': 29, '86': 30, '87': 31, '88': 32, '90': 33, '93': 34, '100': 35, '10': 36, '12': 37, '14': 38, '16': 39, '20': 40, '22': 41, '24': 42, '21': 43, '26': 44, '31': 45, '36': 46, '41': 47, '46': 48, '63': 49, '69': 50, '81': 51, '99': 52, '70': 53, '72': 54, '73': 55, '78': 56, '1': 57, '8': 58, '15': 59, '29': 60, '57': 61, '5': 62, '7': 63, '9': 64, '11': 65, '13': 66, '6': 67, '55': 68, '30': 69, '39': 70, '48': 71, '66': 72, '84': 73, '34': 74, '37': 75, '40': 76, '49': 77, '52': 78, '58': 79, '64': 80, '91': 81, '25': 82, '79': 83, '97': 84, '54': 85, '27': 86, '32': 87, '35': 88, '17': 89, '45': 90, '42': 91, '92': 92, '19': 93, '98': 94, '4': 95, '3': 96, '51': 97, '44': 98, '60': 99, '2': 100, '0': 101, 'generate': 102, 'an': 103, 'arithmetic': 104, 'series': 105, 'with': 106, 'terms': 107, 'starting': 108, 'value': 109, 'and': 110, 'common': 111, 'difference': 112, 'Let': 113, 'be': 114, 'the': 115, 'number': 116, 'of': 117, 'then': 118, 'write': 119, 'make': 120, 'a': 121, 'type': 122, 'which': 123, 'starts': 124, 'at': 125, 'elements': 126, '<PAD>': 127, '<EOS>': 128} \n",
            "\n",
            "index2token: {0: '18', 1: '23', 2: '28', 3: '33', 4: '38', 5: '43', 6: '<SEP>', 7: '67', 8: '76', 9: '85', 10: '94', 11: '61', 12: '68', 13: '75', 14: '82', 15: '89', 16: '96', 17: '47', 18: '53', 19: '59', 20: '65', 21: '71', 22: '77', 23: '83', 24: '95', 25: '50', 26: '56', 27: '62', 28: '74', 29: '80', 30: '86', 31: '87', 32: '88', 33: '90', 34: '93', 35: '100', 36: '10', 37: '12', 38: '14', 39: '16', 40: '20', 41: '22', 42: '24', 43: '21', 44: '26', 45: '31', 46: '36', 47: '41', 48: '46', 49: '63', 50: '69', 51: '81', 52: '99', 53: '70', 54: '72', 55: '73', 56: '78', 57: '1', 58: '8', 59: '15', 60: '29', 61: '57', 62: '5', 63: '7', 64: '9', 65: '11', 66: '13', 67: '6', 68: '55', 69: '30', 70: '39', 71: '48', 72: '66', 73: '84', 74: '34', 75: '37', 76: '40', 77: '49', 78: '52', 79: '58', 80: '64', 81: '91', 82: '25', 83: '79', 84: '97', 85: '54', 86: '27', 87: '32', 88: '35', 89: '17', 90: '45', 91: '42', 92: '92', 93: '19', 94: '98', 95: '4', 96: '3', 97: '51', 98: '44', 99: '60', 100: '2', 101: '0', 102: 'generate', 103: 'an', 104: 'arithmetic', 105: 'series', 106: 'with', 107: 'terms', 108: 'starting', 109: 'value', 110: 'and', 111: 'common', 112: 'difference', 113: 'Let', 114: 'be', 115: 'the', 116: 'number', 117: 'of', 118: 'then', 119: 'write', 120: 'make', 121: 'a', 122: 'type', 123: 'which', 124: 'starts', 125: 'at', 126: 'elements', 127: '<PAD>', 128: '<EOS>'} \n",
            "\n",
            "seq_token: ['18', '23', '28', '33', '38', '43', '<SEP>', '67', '76', '85'] \n",
            "\n",
            "seq_ind: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \n",
            "\n",
            "seq_token: ['18', '23', '28', '33', '38', '43', '<SEP>', '67', '76', '85'] \n",
            "\n",
            "number of tokens in the sequence : 3000000 \n",
            "\n",
            "save_file_dictionary: dataset/step1_02_SSL_dictionary_23-12-05--01-02-01.pt \n",
            "\n",
            "save_file_seq: dataset/step1_03_SSL_training_set_int_23-12-05--01-02-01.pt \n",
            "\n"
          ]
        }
      ],
      "source": [
        "save_dictionary = False\n",
        "save_dictionary = True\n",
        "if save_dictionary:\n",
        "\n",
        "    # create the dictionary of tokens by extracting unique tokens (words)\n",
        "    load_file = data_dir + '/step1_01_SSL_training_set_token_' + time_stamp + '.pt'\n",
        "    print('load_file:', save_file, '\\n')\n",
        "    print('number of tokens in the sequence :',len(seq_tokens),'\\n')\n",
        "    dictionary = []\n",
        "    num_tokens = 0\n",
        "    for token in seq_tokens:\n",
        "        if token not in dictionary:\n",
        "            dictionary.append(token)\n",
        "            num_tokens += 1\n",
        "    print('dictionary:',dictionary,'\\n')\n",
        "    print('num_tokens (unique):',num_tokens,'\\n')\n",
        "\n",
        "    # add tokens to the dictionary for step #2\n",
        "    tokens_for_step2 = ['generate', 'an', 'arithmetic', 'series', 'with', 'terms', 'starting', 'value', \\\n",
        "                        'and', 'common', 'difference', 'Let', 'be', 'the', 'number', 'of', 'then', 'write', \\\n",
        "                        'make', 'a', 'type', 'which', 'starts', 'at', 'elements', '<PAD>', '<EOS>']\n",
        "    for token in torch.arange(m).tolist(): tokens_for_step2.append(str(token))\n",
        "    # update dictionary\n",
        "    for token in tokens_for_step2:\n",
        "        if token not in dictionary:\n",
        "            dictionary.append(token); num_tokens += 1\n",
        "    print('updated dictionary:',dictionary,'\\n')\n",
        "    print('num_tokens (unique):',num_tokens,'\\n')\n",
        "\n",
        "    # token2index : dict w/ key=token(str) and value=index(int)\n",
        "    # index2token : dict w/ key=index(int) and value=token(str)\n",
        "    token2index = { token:index for index,token in enumerate(dictionary) }\n",
        "    index2token = { index:token for index,token in enumerate(dictionary) }\n",
        "    print('token2index:', token2index,'\\n')\n",
        "    print('index2token:', index2token,'\\n')\n",
        "\n",
        "    # func_tokens2indices : function that converts token (str) to indices (int) for token embedding\n",
        "    # func_indices2tokens : function that converts indices (int) to token (str)\n",
        "    # func_str2tokens : function that converts a string into tokens (str)\n",
        "    # func_tokens2str : function that converts tokens (str) to a string\n",
        "    func_tokens2indices = lambda list_tokens: [token2index[token] for token in list_tokens] # ['Let', '5', 'be', 'the'] => [113, 46, 114, 115]\n",
        "    func_indices2tokens = lambda list_ints: [index2token[integer] for integer in list_ints] # [113, 46, 114, 115] => ['Let', '5', 'be', 'the']\n",
        "    func_str2tokens = lambda input_str: [token_str for token_str in input_str.split()]      # 'Let 5 be the' => ['Let', '5', 'be', 'the']\n",
        "    func_tokens2str = lambda list_str: ' '.join(list_str)                                   # ['Let', '5', 'be', 'the'] => 'Let 5 be the'\n",
        "\n",
        "    # example\n",
        "    seq_token = seq_tokens[:10] # first tokens\n",
        "    print('seq_token:', seq_token,'\\n')\n",
        "    seq_ind = func_tokens2indices(seq_token) # token (str) to indices (int)\n",
        "    print('seq_ind:', seq_ind,'\\n')\n",
        "    seq_token = func_indices2tokens(seq_ind) # indices (int) to token (str)\n",
        "    print('seq_token:', seq_token,'\\n')\n",
        "\n",
        "    # convert long seq from tokens to torch integers for training\n",
        "    seq = torch.tensor(func_tokens2indices(seq_tokens))\n",
        "    print('number of tokens in the sequence :',seq.size(0),'\\n')\n",
        "\n",
        "    # save dictionary and training data\n",
        "    save_file_dictionary = data_dir + '/step1_02_SSL_dictionary_' + time_stamp + '.pt'\n",
        "    print('save_file_dictionary:', save_file_dictionary, '\\n')\n",
        "    torch.save([dictionary, num_tokens, token2index, index2token], save_file_dictionary) # save dictionary of tokens\n",
        "    save_file_seq = data_dir + '/step1_03_SSL_training_set_int_' + time_stamp + '.pt'\n",
        "    print('save_file_seq:', save_file_seq, '\\n')\n",
        "    torch.save([seq], save_file_seq) # save the sequence of integers\n",
        "\n",
        "else:\n",
        "\n",
        "    # load dictionary and training data\n",
        "    load_file_dictionary = data_dir + '/step1_02_SSL_dictionary_' + time_stamp + '.pt'\n",
        "    print('load_file_dictionary:', load_file_dictionary, '\\n')\n",
        "    dictionary, num_tokens, token2index, index2token = torch.load(load_file_dictionary) # load dictionary of tokens\n",
        "    load_file_seq = data_dir + '/step1_03_SSL_training_set_int_' + time_stamp + '.pt'\n",
        "    print('load_file_seq:', load_file_seq, '\\n')\n",
        "    seq = torch.load(load_file_seq)[0] # load the sequence of integers\n",
        "\n",
        "    # print\n",
        "    print('dictionary:',dictionary,'\\n')\n",
        "    print('num_tokens (unique):',num_tokens,'\\n')\n",
        "    print('token2index:', token2index,'\\n')\n",
        "    print('index2token:', index2token,'\\n')\n",
        "    print('number of tokens in the sequence :',len(seq),'\\n')\n",
        "    func_tokens2indices = lambda list_tokens: [token2index[token] for token in list_tokens] # ['Let', '5', 'be', 'the'] => [113, 46, 114, 115]\n",
        "    func_indices2tokens = lambda list_ints: [index2token[integer] for integer in list_ints] # [113, 46, 114, 115] => ['Let', '5', 'be', 'the']\n",
        "    func_str2tokens = lambda input_str: [token_str for token_str in input_str.split()]      # 'Let 5 be the' => ['Let', '5', 'be', 'the']\n",
        "    func_tokens2str = lambda list_str: ' '.join(list_str)                                   # ['Let', '5', 'be', 'the'] => 'Let 5 be the'\n",
        "\n",
        "    # example\n",
        "    seq_token = seq_tokens[:10] # first tokens\n",
        "    print('seq_token:', seq_token,'\\n')\n",
        "    seq_ind = func_tokens2indices(seq_token) # token (str) to indices (int)\n",
        "    print('seq_ind:', seq_ind,'\\n')\n",
        "    seq_token = func_indices2tokens(seq_ind) # indices (int) to token (str)\n",
        "    print('seq_token:', seq_token,'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8MsUF2EBfiv"
      },
      "source": [
        "## Get batch of sub-sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0GKflbxm5Jk",
        "outputId": "ee70c593-d461-44ff-ad57-aea06ec0a642",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_len: 3000000, batch_size: 100, batch_length: 100, num_subseq: 30000, num_batch: 300\n",
            "\n",
            "list_batch_idx (before) : tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299]) torch.Size([300]) \n",
            "\n",
            "batch_seq               : tensor([[59, 40, 82,  ..., 97, 99, 50],\n",
            "        [60, 46,  5,  ..., 50, 13, 51],\n",
            "        [88, 46, 75,  ..., 24,  6, 60],\n",
            "        ...,\n",
            "        [71, 97, 85,  ..., 41, 82,  2],\n",
            "        [ 6,  3,  4,  ..., 41, 86, 87],\n",
            "        [11, 72, 21,  ..., 76, 47, 91]]) torch.Size([100, 100]) \n",
            "\n",
            "target_seq              : tensor([[40, 82, 69,  ..., 99, 50, 56],\n",
            "        [46,  5, 25,  ..., 13, 51,  6],\n",
            "        [46, 75,  4,  ...,  6, 60, 69],\n",
            "        ...,\n",
            "        [97, 85, 61,  ..., 82,  2, 45],\n",
            "        [ 3,  4,  5,  ..., 86, 87, 75],\n",
            "        [72, 21,  8,  ..., 47, 91,  5]]) torch.Size([100, 100]) \n",
            "\n",
            "list_batch_idx (after)  : tensor([  3,   4,   6,   9,  10,  11,  12,  13,  14,  15,  18,  20,  22,  24,\n",
            "         25,  27,  28,  29,  30,  31,  32,  34,  36,  37,  38,  39,  40,  42,\n",
            "         43,  46,  47,  49,  51,  52,  54,  55,  58,  59,  60,  62,  63,  64,\n",
            "         65,  66,  68,  69,  70,  73,  74,  75,  77,  78,  79,  83,  84,  85,\n",
            "         87,  89,  90,  91,  92,  96,  98, 104, 105, 106, 107, 109, 110, 112,\n",
            "        113, 114, 115, 116, 117, 118, 119, 122, 124, 126, 128, 129, 130, 131,\n",
            "        133, 135, 136, 140, 143, 144, 145, 146, 148, 149, 151, 152, 155, 156,\n",
            "        157, 158, 159, 160, 161, 164, 166, 168, 170, 171, 173, 175, 177, 178,\n",
            "        179, 180, 181, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197,\n",
            "        198, 199, 201, 202, 204, 206, 207, 208, 211, 212, 214, 215, 216, 217,\n",
            "        218, 219, 223, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237,\n",
            "        238, 239, 240, 242, 243, 244, 246, 247, 248, 249, 252, 253, 254, 255,\n",
            "        256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267, 271, 272, 273,\n",
            "        274, 275, 278, 279, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292,\n",
            "        293, 294, 297, 299]) torch.Size([200]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prepare batch of sub-sequences of the long sequence of tokens\n",
        "#\n",
        "#                              seq_len\n",
        "#                  ------------------------------\n",
        "# seq            = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, ... ]\n",
        "#                       |<= start_idx = 2 (randomly selected in [0,1,...,batch_length-1])\n",
        "#                       -------\n",
        "#                     batch_length = 3 tokens\n",
        "#\n",
        "# batch_seq      = [ [0, 6, 3],   |\n",
        "#                    [2, 9, 5],   | batch_size\n",
        "#                    [2, 3, 4] ]  |\n",
        "#                     -------\n",
        "#                   batch_length\n",
        "#\n",
        "# batch_target =   [ [6, 3, 8],   |\n",
        "#  = batch_seq + 1   [9, 5, 6],   | batch_size\n",
        "#                    [3, 4, 5] ]  |\n",
        "#                     -------\n",
        "#                   batch_length\n",
        "#\n",
        "\n",
        "# parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 3; batch_length = 6 # debug\n",
        "batch_size = 100; batch_length = 100 # GPU\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# create batch of sub-sequences\n",
        "def get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx):\n",
        "    batch_idx = torch.randperm(list_batch_idx.size(0))[:batch_size] # sample \"batch_size\" number of batch indices\n",
        "    batch_idx = list_batch_idx[batch_idx] # sample from remaining list of batch indices\n",
        "    batch_seq = torch.stack([seq[start_idx+i*batch_length : start_idx+(i+1)*batch_length] for i in batch_idx]) # extract batch at start_idx with batch_length, size=[batch_size, batch_length]\n",
        "    target_seq = torch.stack([seq[start_idx+i*batch_length+1 : start_idx+(i+1)*batch_length+1] for i in batch_idx]) # target = batch_seq shifted by +1 to predict next token, size=[batch_size, batch_length]\n",
        "    if list_batch_idx.size(0) > batch_size:\n",
        "        list_batch_idx = torch.stack([i for i in list_batch_idx if i not in batch_idx]) # remove sampled batch indices from the list of batch indices, size=[rem_num_batch_indices]\n",
        "    else:\n",
        "        list_batch_idx = torch.tensor([]) # last batch of epoch, size=[] (empty tensor)\n",
        "    return batch_seq, target_seq, list_batch_idx\n",
        "\n",
        "# print example\n",
        "for _ in range(1): # e.g. num_batch for one full epoch\n",
        "    print('list_batch_idx (before) :',list_batch_idx, list_batch_idx.size(),'\\n')\n",
        "    batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "    print('batch_seq               :',batch_seq, batch_seq.size(),'\\n')\n",
        "    print('target_seq              :',target_seq, target_seq.size(),'\\n')\n",
        "    print('list_batch_idx (after)  :',list_batch_idx, list_batch_idx.size(),'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbwmjcxcBfiw"
      },
      "source": [
        "## Define class of token embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVoON3UGm5Jl",
        "outputId": "5fd94088-54e0-4c76-dd36-39326cf44c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_int : torch.Size([100, 100])\n",
            "batch_vec : torch.Size([100, 100, 128])\n"
          ]
        }
      ],
      "source": [
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# print example\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "list_batch_idx = torch.arange(num_batch) # size=[batch_size]\n",
        "batch_int, _, _ = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch, size=[batch_size, batch_length]\n",
        "print('batch_int :',batch_int.size())\n",
        "\n",
        "token2vec_layer = token2vec(num_tokens, d=128)\n",
        "batch_vec = token2vec_layer(batch_int) # size=[batch_size, batch_length, d=128]\n",
        "print('batch_vec :',batch_vec.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO9EG2q-Bfiw"
      },
      "source": [
        "## Vanilla LM : Predict next token(t+1) given context = {current token : token(t)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pIrC_INm5Jm",
        "outputId": "992c6381-5279-46e5-f785-4e25d40a18c5",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128\n",
            "\n",
            "num_net_parameters: 32896 / 0.03 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.062, lr= 0.000300, loss_epoch: 4.982\n",
            "Epoch: 10, time(sec): 0.084, lr= 0.000300, loss_epoch: 4.766\n",
            "Epoch: 20, time(sec): 0.100, lr= 0.000300, loss_epoch: 4.504\n",
            "Epoch: 30, time(sec): 0.114, lr= 0.000300, loss_epoch: 4.216\n",
            "Epoch: 40, time(sec): 0.130, lr= 0.000300, loss_epoch: 3.969\n",
            "Epoch: 50, time(sec): 0.147, lr= 0.000300, loss_epoch: 3.732\n",
            "Epoch: 60, time(sec): 0.164, lr= 0.000300, loss_epoch: 3.560\n",
            "Epoch: 70, time(sec): 0.177, lr= 0.000300, loss_epoch: 3.353\n",
            "Epoch: 80, time(sec): 0.190, lr= 0.000300, loss_epoch: 3.117\n",
            "Epoch: 90, time(sec): 0.213, lr= 0.000300, loss_epoch: 2.897\n",
            "Epoch: 100, time(sec): 0.233, lr= 0.000300, loss_epoch: 2.765\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "#    seq = [ 2, 3, 4, 5, 6 ]\n",
        "#               - <= context = { current token } = 3\n",
        "#               | <= predict next token = 4\n",
        "# target = [ 3, 4, 5, 6, 7 ]\n",
        "#               | <= score vector v must predict token \"4\"\n",
        "# scores = [ v, v, v, v, v ]\n",
        "#\n",
        "# batch_seq      = [ [ 2, 3, 4, 5, 6 ],   |\n",
        "#                    [ 2, 9, 5, 7, 3 ],   | batch_size\n",
        "#                    [ 0, 6, 3, 5, 2 ] ]  |\n",
        "#                     --------------\n",
        "#                       batch_length\n",
        "#\n",
        "# batch_scores =   [ [ v, v, v, v, v ],   |\n",
        "#                    [ v, v, v, v, v ],   | batch_size, v is a vector of \"num_tokens\" dimensions\n",
        "#                    [ v, v, v, v, v ] ]  |\n",
        "#                     --------------\n",
        "#                      batch_length\n",
        "#\n",
        "# batch_target =   [ [ 3, 4, 5, 6, 7 ],   |\n",
        "#  = batch_seq + 1   [ 9, 5, 7, 3, 1 ],   | batch_size\n",
        "#                    [ 6, 3, 5, 2, 8 ] ]  |\n",
        "#                     --------------\n",
        "#                      batch_length\n",
        "#\n",
        "class vanillaLM(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        batch_seq_vec = self.token2vec(batch_seq) # size=[batch_size, batch_length, d]\n",
        "        batch_scores = self.token_prediction(batch_seq_vec) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "print('num_tokens: %d, d: %d\\n' % (num_tokens, d) )\n",
        "vanillaLMnet = vanillaLM(num_tokens, d)\n",
        "num_param = number_param(vanillaLMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(vanillaLMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = vanillaLMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qb1TZj5Bfix"
      },
      "source": [
        "## Bag-Of-Token LM : Predict next token(t+1) given context = {bag of tokens : token(<=t)}\n",
        "### Aggregator of tokens is the mean operator (as a bag-of-token is a set of unordered tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSsCq4qCm5Jm",
        "outputId": "0a876b19-4122-43da-c4a0-a7e8bf804aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128\n",
            "\n",
            "num_net_parameters: 32896 / 0.03 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.004, lr= 0.000300, loss_epoch: 4.862\n",
            "Epoch: 10, time(sec): 0.040, lr= 0.000300, loss_epoch: 4.763\n",
            "Epoch: 20, time(sec): 0.066, lr= 0.000300, loss_epoch: 4.674\n",
            "Epoch: 30, time(sec): 0.087, lr= 0.000300, loss_epoch: 4.585\n",
            "Epoch: 40, time(sec): 0.108, lr= 0.000300, loss_epoch: 4.436\n",
            "Epoch: 50, time(sec): 0.126, lr= 0.000300, loss_epoch: 4.419\n",
            "Epoch: 60, time(sec): 0.143, lr= 0.000300, loss_epoch: 4.331\n",
            "Epoch: 70, time(sec): 0.159, lr= 0.000300, loss_epoch: 4.258\n",
            "Epoch: 80, time(sec): 0.177, lr= 0.000300, loss_epoch: 4.164\n",
            "Epoch: 90, time(sec): 0.194, lr= 0.000300, loss_epoch: 4.067\n",
            "Epoch: 100, time(sec): 0.219, lr= 0.000300, loss_epoch: 4.025\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "#    seq = [ 2, 3, 4, 5, 6 ]\n",
        "#            ---------- <= context = { tokens(<=t) } = 2,3,4,5\n",
        "#                     | <= predict next token = 6\n",
        "# target = [ 3, 4, 5, 6, 7 ]\n",
        "#                     | <= score vector v must predict token \"6\"\n",
        "# scores = [ v, v, v, v, v ]\n",
        "#\n",
        "# triu(ones(3,3)) = [ [1, 0, 0]\n",
        "#                     [1, 1, 0]\n",
        "#                     [1, 1, 1] ]\n",
        "#\n",
        "class BOT_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        batch_size = batch_seq.size(0); batch_len = batch_seq.size(1)\n",
        "        batch_seq_vec = self.token2vec(batch_seq) # size=[batch_size, batch_length, d]\n",
        "        mean_operator = torch.tril(torch.ones(batch_len,batch_len)).long() # mask to use previous tokens only : { token(<=t) }, size=[batch_len,batch_len]\n",
        "        mean_operator = mean_operator/ torch.sum(mean_operator, dim=1).unsqueeze(1) # normalize w.r.t. number of previous tokens\n",
        "        mean_operator = mean_operator.repeat(batch_size,1,1) # repeat masks batch_size times, size=(batch_size, batch_len, batch_len)\n",
        "        batch_seq_vec =  mean_operator @ batch_seq_vec # matrix-matrix multiplication (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, num_tokens)\n",
        "        batch_scores = self.token_prediction(batch_seq_vec) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "print('num_tokens: %d, d: %d\\n' % (num_tokens, d) )\n",
        "BOT_LMnet = BOT_LM(num_tokens, d)\n",
        "num_param = number_param(BOT_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(BOT_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = BOT_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8soVaZsDBfiy"
      },
      "source": [
        "## Vanilla Self-Attention LM : Predict next token(t+1) given context = {token(<=t)}\n",
        "### Aggregator of tokens is the self-attention operator : $\\textrm{softmax}( HH^T / \\sqrt{d} ) H$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilni3dKfm5Jn",
        "outputId": "5bdbacf4-994a-44d0-a832-0c2a0153f038",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128\n",
            "\n",
            "num_net_parameters: 32896 / 0.03 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.004, lr= 0.000300, loss_epoch: 4.982\n",
            "Epoch: 10, time(sec): 0.040, lr= 0.000300, loss_epoch: 4.766\n",
            "Epoch: 20, time(sec): 0.062, lr= 0.000300, loss_epoch: 4.504\n",
            "Epoch: 30, time(sec): 0.089, lr= 0.000300, loss_epoch: 4.216\n",
            "Epoch: 40, time(sec): 0.106, lr= 0.000300, loss_epoch: 3.969\n",
            "Epoch: 50, time(sec): 0.121, lr= 0.000300, loss_epoch: 3.732\n",
            "Epoch: 60, time(sec): 0.136, lr= 0.000300, loss_epoch: 3.560\n",
            "Epoch: 70, time(sec): 0.152, lr= 0.000300, loss_epoch: 3.354\n",
            "Epoch: 80, time(sec): 0.167, lr= 0.000300, loss_epoch: 3.117\n",
            "Epoch: 90, time(sec): 0.183, lr= 0.000300, loss_epoch: 2.898\n",
            "Epoch: 100, time(sec): 0.198, lr= 0.000300, loss_epoch: 2.765\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "class VSA_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        batch_size = batch_seq.size(0); batch_len = batch_seq.size(1)\n",
        "        H = self.token2vec(batch_seq) # size=[batch_size, batch_length, d]\n",
        "        attention_score = H @ H.transpose(2,1) * H.size(2)**-0.5 # HH^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
        "        mask = torch.tril(torch.ones(batch_len,batch_len)).long() # mask to use previous tokens only : { token(<=t) }, size=[batch_len,batch_len]\n",
        "        attention_score = attention_score.masked_fill(mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
        "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n",
        "        batch_seq_vec = attention_score @ H # softmax( HH^T / sqrt(d) ) H, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n",
        "        batch_scores = self.token_prediction(batch_seq_vec) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "print('num_tokens: %d, d: %d\\n' % (num_tokens, d) )\n",
        "VSA_LMnet = VSA_LM(num_tokens, d)\n",
        "num_param = number_param(VSA_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(VSA_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = VSA_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wOV9jr8Bfiy"
      },
      "source": [
        "## Standard Self-Attention LM : Predict next token(t+1) given context = {token(<=t)}\n",
        "### Aggregator of tokens is the self-attention operator : $\\textrm{softmax}( QK^T / \\sqrt{d} ) V$\n",
        "### with learnable dictionary Q=Query, K=Key, V=Value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qYkVwz6m5Jo",
        "outputId": "5bf92bf7-c1f4-49c8-93af-d5b34f2075b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128\n",
            "\n",
            "num_net_parameters: 82176 / 0.08 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.005, lr= 0.000300, loss_epoch: 4.836\n",
            "Epoch: 10, time(sec): 0.049, lr= 0.000300, loss_epoch: 4.691\n",
            "Epoch: 20, time(sec): 0.083, lr= 0.000300, loss_epoch: 4.409\n",
            "Epoch: 30, time(sec): 0.106, lr= 0.000300, loss_epoch: 4.104\n",
            "Epoch: 40, time(sec): 0.129, lr= 0.000300, loss_epoch: 3.859\n",
            "Epoch: 50, time(sec): 0.152, lr= 0.000300, loss_epoch: 3.456\n",
            "Epoch: 60, time(sec): 0.177, lr= 0.000300, loss_epoch: 3.192\n",
            "Epoch: 70, time(sec): 0.201, lr= 0.000300, loss_epoch: 2.951\n",
            "Epoch: 80, time(sec): 0.233, lr= 0.000300, loss_epoch: 2.718\n",
            "Epoch: 90, time(sec): 0.261, lr= 0.000300, loss_epoch: 2.662\n",
            "Epoch: 100, time(sec): 0.283, lr= 0.000300, loss_epoch: 2.328\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "class SA_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.query = nn.Linear(d, d, bias=False) # query embedding layer\n",
        "        self.key = nn.Linear(d, d, bias=False) # key embedding layer\n",
        "        self.value = nn.Linear(d, d) # value embedding layer\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        batch_size = batch_seq.size(0); batch_len = batch_seq.size(1)\n",
        "        H = self.token2vec(batch_seq) # size=[batch_size, batch_length, d]\n",
        "        Q = self.query(H) # size=[batch_size, batch_length, d]\n",
        "        K = self.key(H) # size=[batch_size, batch_length, d]\n",
        "        V = self.value(H) # size=[batch_size, batch_length, d]\n",
        "        attention_score = Q @ K.transpose(2,1) * H.size(2)**-0.5 # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
        "        mask = torch.tril(torch.ones(batch_len,batch_len)).long() # mask to use previous tokens only : { token(<=t) }, size=[batch_len,batch_len]\n",
        "        attention_score = attention_score.masked_fill(mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
        "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n",
        "        batch_seq_vec = attention_score @ V # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n",
        "        batch_scores = self.token_prediction(batch_seq_vec) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "print('num_tokens: %d, d: %d\\n' % (num_tokens, d) )\n",
        "SA_LMnet = SA_LM(num_tokens, d)\n",
        "num_param = number_param(SA_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(SA_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = SA_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUzXYDPLBfiz"
      },
      "source": [
        "## Self-Attention LM : Predict next token(t+1) given context = {token(<=t)}\n",
        "## Add positional encoding (PE) to self-attention / (single) attention head\n",
        "###  PE is required to add ordering information to token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNRmSmxfm5Jo",
        "outputId": "84d1db5e-ee16-4d70-e420-8ddca1944ac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128, batch_length: 20\n",
            "\n",
            "num_net_parameters: 84736 / 0.08 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.007, lr= 0.000300, loss_epoch: 4.904\n",
            "Epoch: 10, time(sec): 0.053, lr= 0.000300, loss_epoch: 4.706\n",
            "Epoch: 20, time(sec): 0.086, lr= 0.000300, loss_epoch: 4.494\n",
            "Epoch: 30, time(sec): 0.112, lr= 0.000300, loss_epoch: 4.179\n",
            "Epoch: 40, time(sec): 0.136, lr= 0.000300, loss_epoch: 3.960\n",
            "Epoch: 50, time(sec): 0.160, lr= 0.000300, loss_epoch: 3.759\n",
            "Epoch: 60, time(sec): 0.183, lr= 0.000300, loss_epoch: 3.465\n",
            "Epoch: 70, time(sec): 0.208, lr= 0.000300, loss_epoch: 3.106\n",
            "Epoch: 80, time(sec): 0.235, lr= 0.000300, loss_epoch: 3.159\n",
            "Epoch: 90, time(sec): 0.264, lr= 0.000300, loss_epoch: 2.797\n",
            "Epoch: 100, time(sec): 0.286, lr= 0.000300, loss_epoch: 2.823\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# single head attention layer\n",
        "class head_attention(nn.Module):\n",
        "    def __init__(self, d, context_length):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(d, d, bias=False) # query embedding layer\n",
        "        self.key = nn.Linear(d, d, bias=False) # key embedding layer\n",
        "        self.value = nn.Linear(d, d) # value embedding layer\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length)).long() # mask to use previous tokens only : { token(<=t) }, size=[context_length, context_length]\n",
        "    def forward(self, H):\n",
        "        Q = self.query(H) # size=[batch_size, batch_length, d]\n",
        "        K = self.key(H) # size=[batch_size, batch_length, d]\n",
        "        V = self.value(H) # size=[batch_size, batch_length, d]\n",
        "        attention_score = Q @ K.transpose(2,1) * H.size(2)**-0.5 # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
        "        attention_score = attention_score.masked_fill(self.mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
        "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len]\n",
        "        H_head = attention_score @ V # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d]\n",
        "        return H_head\n",
        "\n",
        "class PE_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.HA = head_attention(d, context_length) # self-attention layer\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        seq_pos_encoding = torch.arange(batch_seq.size(1)) # positional encoding = {0,1,2,...,batch_length-1}\n",
        "        H = self.token2vec(batch_seq) + self.PE_embedding(seq_pos_encoding).unsqueeze(0) # size=[batch_size, batch_length, d]\n",
        "        batch_seq_vec = self.HA(H) # (single) attention head, size=[batch_size, batch_length, d]\n",
        "        batch_scores = self.token_prediction(batch_seq_vec) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "print('num_tokens: %d, d: %d, batch_length: %d\\n' % (num_tokens, d, batch_length) )\n",
        "PE_LMnet = PE_LM(num_tokens, d, batch_length)\n",
        "num_param = number_param(PE_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(PE_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = PE_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwR5x0GDBfiz"
      },
      "source": [
        "## LM with Multiple Attention Heads  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5oV00NDm5Jp",
        "outputId": "640671cf-830a-4ac7-9365-b08fa68218c2",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128, batch_length: 20, num_heads: 16\n",
            "\n",
            "num_net_parameters: 101248 / 0.10 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.027, lr= 0.000300, loss_epoch: 4.851\n",
            "Epoch: 10, time(sec): 0.163, lr= 0.000300, loss_epoch: 4.662\n",
            "Epoch: 20, time(sec): 0.301, lr= 0.000300, loss_epoch: 4.352\n",
            "Epoch: 30, time(sec): 0.426, lr= 0.000300, loss_epoch: 4.190\n",
            "Epoch: 40, time(sec): 0.564, lr= 0.000300, loss_epoch: 3.760\n",
            "Epoch: 50, time(sec): 0.698, lr= 0.000300, loss_epoch: 3.664\n",
            "Epoch: 60, time(sec): 0.829, lr= 0.000300, loss_epoch: 3.652\n",
            "Epoch: 70, time(sec): 0.948, lr= 0.000300, loss_epoch: 3.366\n",
            "Epoch: 80, time(sec): 1.072, lr= 0.000300, loss_epoch: 3.095\n",
            "Epoch: 90, time(sec): 1.199, lr= 0.000300, loss_epoch: 3.087\n",
            "Epoch: 100, time(sec): 1.323, lr= 0.000300, loss_epoch: 2.884\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# single head attention layer\n",
        "class head_attention(nn.Module):\n",
        "    def __init__(self, d, d_head, context_length):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n",
        "        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n",
        "        self.value = nn.Linear(d, d_head) # value embedding layer\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length)).long() # mask to use previous tokens only : { token(<=t) }, size=[context_length, context_length]\n",
        "    def forward(self, H):\n",
        "        Q = self.query(H) # size=[batch_size, batch_length, d_head]\n",
        "        K = self.key(H) # size=[batch_size, batch_length, d_head]\n",
        "        V = self.value(H) # size=[batch_size, batch_length, d_head]\n",
        "        attention_score = Q @ K.transpose(2,1) * H.size(2)**-0.5 # QK^T/sqrt(d_head), (B,L,d_head) @ (B,d_head,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
        "        attention_score = attention_score.masked_fill(self.mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
        "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len]\n",
        "        H_head = attention_score @ V # softmax( QK^T / sqrt(d_head) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d_head]\n",
        "        return H_head\n",
        "\n",
        "# multiple attention heads layer\n",
        "class multiple_head_attention(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads):\n",
        "        super().__init__()\n",
        "        d_head = d // num_heads # dim_head = d / num_heads, usually dimension per head is 64\n",
        "        assert d == d_head * num_heads # check divisibility\n",
        "        self.MHA = nn.ModuleList([ head_attention(d, d_head, context_length) for _ in range(num_heads) ])\n",
        "        self.combined_heads = nn.Linear(d, d) # combination layer\n",
        "    def forward(self, H):\n",
        "        H_heads = []\n",
        "        for HA_layer in self.MHA:\n",
        "            H_heads.append(HA_layer(H)) # size=[batch_size, batch_length, d_head]\n",
        "        H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, batch_length, d]\n",
        "        H_heads = self.combined_heads(H_heads) # size=[batch_size, batch_length, d]\n",
        "        return H_heads\n",
        "\n",
        "class MHA_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length, num_heads):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.MHA = multiple_head_attention(d, context_length, num_heads) # multiple self-attention layers\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        seq_pos_encoding = torch.arange(batch_seq.size(1)) # positional encoding = {0,1,2,...,batch_length-1}\n",
        "        H = self.token2vec(batch_seq) + self.PE_embedding(seq_pos_encoding).unsqueeze(0) # size=[batch_size, batch_length, d]\n",
        "        batch_seq_vec = self.MHA(H) # (single) attention head, size=[batch_size, batch_length, d]\n",
        "        batch_scores = self.token_prediction(batch_seq_vec) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "num_heads = 16\n",
        "print('num_tokens: %d, d: %d, batch_length: %d, num_heads: %d\\n' % (num_tokens, d, batch_length, num_heads) )\n",
        "MHA_LMnet = MHA_LM(num_tokens, d, batch_length, num_heads)\n",
        "num_param = number_param(MHA_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(MHA_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = MHA_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtHsIGiPBfi0"
      },
      "source": [
        "## LM with (single) Transformer Block\n",
        "## Add residual connection (RC) + layer normalization (LN) + dropout + feedforward / MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgnBw8BGm5Jq",
        "outputId": "275fb715-36f2-4424-c916-355f15b97429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128, batch_length: 20, num_heads: 16, dropout: 0.10\n",
            "\n",
            "num_net_parameters: 233472 / 0.23 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.063, lr= 0.000300, loss_epoch: 5.246\n",
            "Epoch: 10, time(sec): 0.285, lr= 0.000300, loss_epoch: 4.359\n",
            "Epoch: 20, time(sec): 0.439, lr= 0.000300, loss_epoch: 3.695\n",
            "Epoch: 30, time(sec): 0.610, lr= 0.000300, loss_epoch: 3.433\n",
            "Epoch: 40, time(sec): 0.784, lr= 0.000300, loss_epoch: 2.746\n",
            "Epoch: 50, time(sec): 0.951, lr= 0.000300, loss_epoch: 2.358\n",
            "Epoch: 60, time(sec): 1.130, lr= 0.000300, loss_epoch: 2.213\n",
            "Epoch: 70, time(sec): 1.300, lr= 0.000300, loss_epoch: 1.916\n",
            "Epoch: 80, time(sec): 1.460, lr= 0.000300, loss_epoch: 1.773\n",
            "Epoch: 90, time(sec): 1.624, lr= 0.000300, loss_epoch: 1.489\n",
            "Epoch: 100, time(sec): 1.783, lr= 0.000300, loss_epoch: 1.338\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# single head attention layer\n",
        "class head_attention(nn.Module):\n",
        "    def __init__(self, d, d_head, context_length, dropout):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n",
        "        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n",
        "        self.value = nn.Linear(d, d_head) # value embedding layer\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length)).long() # mask to use previous tokens only : { token(<=t) }, size=[context_length, context_length]\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, H):\n",
        "        Q = self.query(H) # size=[batch_size, batch_length, d_head]\n",
        "        K = self.key(H) # size=[batch_size, batch_length, d_head]\n",
        "        V = self.value(H) # size=[batch_size, batch_length, d_head]\n",
        "        attention_score = Q @ K.transpose(2,1) * H.size(2)**-0.5 # QK^T/sqrt(d_head), (B,L,d_head) @ (B,d_head,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
        "        attention_score = attention_score.masked_fill(self.mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
        "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len]\n",
        "        attention_score = self.dropout(attention_score) # dropout attention scores\n",
        "        H_head = attention_score @ V # softmax( QK^T / sqrt(d_head) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d_head]\n",
        "        return H_head\n",
        "\n",
        "# multiple attention heads layer\n",
        "class multiple_head_attention(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        d_head = d // num_heads # dim_head = d / num_heads, usually dimension per head is 64\n",
        "        assert d == d_head * num_heads # check divisibility\n",
        "        self.MHA = nn.ModuleList([ head_attention(d, d_head, context_length, dropout) for _ in range(num_heads) ])\n",
        "        self.combined_heads = nn.Linear(d, d) # combination layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, H):\n",
        "        H_heads = []\n",
        "        for HA_layer in self.MHA:\n",
        "            H_heads.append(HA_layer(H)) # size=[batch_size, batch_length, d_head]\n",
        "        H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, batch_length, d]\n",
        "        H_heads = self.dropout(H_heads) # dropout attention activations\n",
        "        H_heads = self.combined_heads(H_heads) # size=[batch_size, batch_length, d]\n",
        "        return H_heads\n",
        "\n",
        "# Transformer block layer\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.MHA = multiple_head_attention(d, context_length, num_heads, dropout)\n",
        "        self.LN_MHA = nn.LayerNorm(d)\n",
        "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n",
        "        self.LN_MLP = nn.LayerNorm(d)\n",
        "    def forward(self, H):\n",
        "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, batch_length, d]\n",
        "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, batch_length, d]\n",
        "        return H\n",
        "\n",
        "class TB_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.TB = TransformerBlock(d, context_length, num_heads, dropout) # transformer block layer\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        seq_pos_encoding = torch.arange(batch_seq.size(1)) # positional encoding = {0,1,2,...,batch_length-1}\n",
        "        H = self.token2vec(batch_seq) + self.PE_embedding(seq_pos_encoding).unsqueeze(0) # size=[batch_size, batch_length, d]\n",
        "        batch_seq_vec = self.TB(H) # (single) transformer block, size=[batch_size, batch_length, d]\n",
        "        batch_scores = self.token_prediction(batch_seq_vec) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "num_heads = 16\n",
        "dropout = 0.1\n",
        "print('num_tokens: %d, d: %d, batch_length: %d, num_heads: %d, dropout: %.2f\\n' % (num_tokens, d, batch_length, num_heads, dropout) )\n",
        "TB_LMnet = TB_LM(num_tokens, d, batch_length, num_heads, dropout)\n",
        "num_param = number_param(TB_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(TB_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = TB_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB36WVpOBfi0"
      },
      "source": [
        "## PyTorch implementation of MHA vs. my implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdxorABkm5Jq",
        "outputId": "e85dc682-dc7c-4331-e848-754fd6c7ccd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128, batch_length: 20, num_heads: 16, dropout: 0.10\n",
            "\n",
            "num_net_parameters: 233728 / 0.23 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.013, lr= 0.000300, loss_epoch: 5.363\n",
            "Epoch: 10, time(sec): 0.079, lr= 0.000300, loss_epoch: 4.508\n",
            "Epoch: 20, time(sec): 0.128, lr= 0.000300, loss_epoch: 3.843\n",
            "Epoch: 30, time(sec): 0.178, lr= 0.000300, loss_epoch: 3.320\n",
            "Epoch: 40, time(sec): 0.228, lr= 0.000300, loss_epoch: 2.939\n",
            "Epoch: 50, time(sec): 0.280, lr= 0.000300, loss_epoch: 2.553\n",
            "Epoch: 60, time(sec): 0.323, lr= 0.000300, loss_epoch: 2.078\n",
            "Epoch: 70, time(sec): 0.370, lr= 0.000300, loss_epoch: 1.795\n",
            "Epoch: 80, time(sec): 0.417, lr= 0.000300, loss_epoch: 1.796\n",
            "Epoch: 90, time(sec): 0.469, lr= 0.000300, loss_epoch: 1.385\n",
            "Epoch: 100, time(sec): 0.519, lr= 0.000300, loss_epoch: 1.091\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# single head attention layer\n",
        "class head_attention(nn.Module):\n",
        "    def __init__(self, d, d_head, context_length, dropout):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n",
        "        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n",
        "        self.value = nn.Linear(d, d_head) # value embedding layer\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length)).long() # mask to use previous tokens only : { token(<=t) }, size=[context_length, context_length]\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, H):\n",
        "        Q = self.query(H) # size=[batch_size, batch_length, d_head]\n",
        "        K = self.key(H) # size=[batch_size, batch_length, d_head]\n",
        "        V = self.value(H) # size=[batch_size, batch_length, d_head]\n",
        "        attention_score = Q @ K.transpose(2,1) * H.size(2)**-0.5 # QK^T/sqrt(d_head), (B,L,d_head) @ (B,d_head,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
        "        attention_score = attention_score.masked_fill(self.mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
        "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len]\n",
        "        attention_score = self.dropout(attention_score) # dropout attention scores\n",
        "        H_head = attention_score @ V # softmax( QK^T / sqrt(d_head) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d_head]\n",
        "        return H_head\n",
        "\n",
        "# # multiple attention heads layer -- my implementation\n",
        "# class multiple_head_attention(nn.Module):\n",
        "#     def __init__(self, d, context_length, num_heads, dropout):\n",
        "#         super().__init__()\n",
        "#         d_head = d // num_heads # dim_head = d / num_heads, usually dimension per head is 64\n",
        "#         assert d == d_head * num_heads # check divisibility\n",
        "#         self.MHA = nn.ModuleList([ head_attention(d, d_head, context_length, dropout) for _ in range(num_heads) ])\n",
        "#         self.combined_heads = nn.Linear(d, d) # combination layer\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#     def forward(self, H):\n",
        "#         H_heads = []\n",
        "#         for HA_layer in self.MHA:\n",
        "#             H_heads.append(HA_layer(H)) # size=[batch_size, batch_length, d_head]\n",
        "#         H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, batch_length, d]\n",
        "#         H_heads = self.dropout(H_heads) # dropout attention activations\n",
        "#         H_heads = self.combined_heads(H_heads) # size=[batch_size, batch_length, d]\n",
        "#         return H_heads\n",
        "\n",
        "# multiple attention heads layer -- PyTorch implementation\n",
        "class multiple_head_attention(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        d_head = d // num_heads\n",
        "        assert d == d_head * num_heads # check divisiblity\n",
        "        self.MHA = nn.MultiheadAttention(d, num_heads, batch_first=True, dropout=dropout)\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length))==0 # mask to make attention to previous tokens only : { token(<=t) }, size=(context_length,context_length)\n",
        "                   # torch.tril(ones) = True in the up-right part, True means *no* attention allowed in pytorch implementation\n",
        "    def forward(self, H):\n",
        "        H_heads = self.MHA(H, H, H, attn_mask=self.mask)[0] # size=[batch_size, batch_length, d]\n",
        "        return H_heads\n",
        "\n",
        "# Transformer block layer\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.MHA = multiple_head_attention(d, context_length, num_heads, dropout)\n",
        "        self.LN_MHA = nn.LayerNorm(d)\n",
        "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n",
        "        self.LN_MLP = nn.LayerNorm(d)\n",
        "    def forward(self, H):\n",
        "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, batch_length, d]\n",
        "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, batch_length, d]\n",
        "        return H\n",
        "\n",
        "class TBpytorch_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.TB = TransformerBlock(d, context_length, num_heads, dropout) # transformer block layer\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        seq_pos_encoding = torch.arange(batch_seq.size(1)) # positional encoding = {0,1,2,...,batch_length-1}\n",
        "        H = self.token2vec(batch_seq) + self.PE_embedding(seq_pos_encoding).unsqueeze(0) # size=[batch_size, batch_length, d]\n",
        "        batch_seq_vec = self.TB(H) # (single) transformer block, size=[batch_size, batch_length, d]\n",
        "        batch_scores = self.token_prediction(batch_seq_vec) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "num_heads = 16\n",
        "dropout = 0.1\n",
        "print('num_tokens: %d, d: %d, batch_length: %d, num_heads: %d, dropout: %.2f\\n' % (num_tokens, d, batch_length, num_heads, dropout) )\n",
        "TB_LMnet = TB_LM(num_tokens, d, batch_length, num_heads, dropout)\n",
        "num_param = number_param(TB_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(TB_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = TB_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n",
        "\n",
        "# my implementation      : Time(sec): 17.707 / loss_epoch: 1.387  => 3x slower\n",
        "# pytorch implementation : Time(sec): 6.556 / loss_epoch: 0.940\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcvM_u-MBfi1"
      },
      "source": [
        "## LM with multiple Transformer Blocks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qCu85e7m5Jr",
        "outputId": "99635dbe-acd4-47ef-b0c4-2bed44b8533f",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128, batch_length: 20, num_heads: 16, dropout: 0.10, num_layers: 2\n",
            "\n",
            "num_net_parameters: 432000 / 0.43 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.023, lr= 0.000300, loss_epoch: 5.076\n",
            "Epoch: 10, time(sec): 0.155, lr= 0.000300, loss_epoch: 4.135\n",
            "Epoch: 20, time(sec): 0.265, lr= 0.000300, loss_epoch: 3.238\n",
            "Epoch: 30, time(sec): 0.364, lr= 0.000300, loss_epoch: 2.606\n",
            "Epoch: 40, time(sec): 0.455, lr= 0.000300, loss_epoch: 2.372\n",
            "Epoch: 50, time(sec): 0.554, lr= 0.000300, loss_epoch: 1.615\n",
            "Epoch: 60, time(sec): 0.651, lr= 0.000300, loss_epoch: 1.386\n",
            "Epoch: 70, time(sec): 0.750, lr= 0.000300, loss_epoch: 1.056\n",
            "Epoch: 80, time(sec): 0.851, lr= 0.000300, loss_epoch: 0.876\n",
            "Epoch: 90, time(sec): 0.948, lr= 0.000300, loss_epoch: 0.875\n",
            "Epoch: 100, time(sec): 1.044, lr= 0.000300, loss_epoch: 0.931\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# multiple attention heads layer\n",
        "class multiple_head_attention(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        d_head = d // num_heads\n",
        "        assert d == d_head * num_heads # check divisiblity\n",
        "        self.MHA = nn.MultiheadAttention(d, num_heads, batch_first=True, dropout=dropout)\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length))==0 # mask to make attention to previous tokens only : { token(<=t) }, size=(context_length,context_length)\n",
        "                   # torch.tril(ones) = True in the up-right part, True means *no* attention allowed in pytorch implementation\n",
        "    def forward(self, H):\n",
        "        H_heads = self.MHA(H, H, H, attn_mask=self.mask)[0] # size=[batch_size, batch_length, d]\n",
        "        return H_heads\n",
        "\n",
        "# Transformer block layer\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.MHA = multiple_head_attention(d, context_length, num_heads, dropout)\n",
        "        self.LN_MHA = nn.LayerNorm(d)\n",
        "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n",
        "        self.LN_MLP = nn.LayerNorm(d)\n",
        "    def forward(self, H):\n",
        "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, batch_length, d]\n",
        "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, batch_length, d]\n",
        "        return H\n",
        "\n",
        "class MTB_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length, num_heads, dropout, num_layers):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.transformer_blocks = nn.ModuleList([ TransformerBlock(d, context_length, num_heads, dropout) for _ in range(num_layers) ]) # multiple transformer block layers\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        seq_pos_encoding = torch.arange(batch_seq.size(1)) # positional encoding = {0,1,2,...,batch_length-1}\n",
        "        H = self.token2vec(batch_seq) + self.PE_embedding(seq_pos_encoding).unsqueeze(0) # size=[batch_size, batch_length, d]\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            H = transformer_block(H) # size=[batch_size, batch_length, d]\n",
        "        batch_scores = self.token_prediction(H) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "num_heads = 16\n",
        "dropout = 0.1\n",
        "num_layers = 2\n",
        "print('num_tokens: %d, d: %d, batch_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, batch_length, num_heads, dropout, num_layers) )\n",
        "MTB_LMnet = MTB_LM(num_tokens, d, batch_length, num_heads, dropout, num_layers)\n",
        "num_param = number_param(MTB_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(MTB_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 101 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = MTB_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vUKK0dBfi1"
      },
      "source": [
        "# Generate a new sequence of any length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnP1SeiOm5Jr",
        "outputId": "142ad588-dc35-4396-aa28-3157990ca26b",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_len: 100, batch_size: 5, batch_length: 20, num_subseq: 5, num_batch: 1\n",
            "\n",
            "num_tokens: 128, d: 128, batch_length: 20, num_heads: 16, dropout: 0.10, num_layers: 2\n",
            "\n",
            "num_net_parameters: 432000 / 0.43 million\n",
            "\n",
            "Epoch: 0, time(sec): 0.018, lr= 0.000300, loss_epoch: 5.076\n",
            "Epoch: 10, time(sec): 0.128, lr= 0.000300, loss_epoch: 4.135\n",
            "\n",
            "sequence   : 70 76 <SEP> 95\n",
            "prediction :                <SEP> <SEP> 90 <SEP> <SEP> <SEP> 99 <SEP> 99 <SEP> 90 <SEP> 90 <SEP> <SEP> 90 <SEP> 99 <SEP> 99 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# multiple attention heads layer\n",
        "class multiple_head_attention(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        d_head = d // num_heads\n",
        "        assert d == d_head * num_heads # check divisiblity\n",
        "        self.MHA = nn.MultiheadAttention(d, num_heads, batch_first=True, dropout=dropout)\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length))==0 # mask to make attention to previous tokens only : { token(<=t) }, size=(context_length,context_length)\n",
        "                   # torch.tril(ones) = True in the up-right part, True means *no* attention allowed in pytorch implementation\n",
        "        self.context_length = context_length\n",
        "    def forward(self, H):\n",
        "        if H.size(1) == self.context_length: # training <==\n",
        "            attn_mask = self.mask\n",
        "        else: # when batch_length not= context_length, e.g. inference time / sequence generation <==\n",
        "            current_batch_length = H.size(1)\n",
        "            attn_mask = torch.tril(torch.ones(current_batch_length, current_batch_length))==0\n",
        "        H_heads = self.MHA(H, H, H, attn_mask=attn_mask)[0] # pytorch implementation, size=[batch_size, batch_length, d]\n",
        "        return H_heads\n",
        "\n",
        "# Transformer block layer\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.MHA = multiple_head_attention(d, context_length, num_heads, dropout)\n",
        "        self.LN_MHA = nn.LayerNorm(d)\n",
        "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n",
        "        self.LN_MLP = nn.LayerNorm(d)\n",
        "    def forward(self, H):\n",
        "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, batch_length, d]\n",
        "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, batch_length, d]\n",
        "        return H\n",
        "\n",
        "class GEN_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length, num_heads, dropout, num_layers):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.transformer_blocks = nn.ModuleList([ TransformerBlock(d, context_length, num_heads, dropout) for _ in range(num_layers) ]) # multiple transformer block layers\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        seq_pos_encoding = torch.arange(batch_seq.size(1)) # positional encoding = {0,1,2,...,batch_length-1}\n",
        "        H = self.token2vec(batch_seq) + self.PE_embedding(seq_pos_encoding).unsqueeze(0) # size=[batch_size, batch_length, d]\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            H = transformer_block(H) # size=[batch_size, batch_length, d]\n",
        "        batch_scores = self.token_prediction(H) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5; batch_length = 20 # bebug\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# network parameters\n",
        "d = 128 # embedding dimension\n",
        "num_heads = 16\n",
        "dropout = 0.1\n",
        "num_layers = 2\n",
        "print('num_tokens: %d, d: %d, batch_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, batch_length, num_heads, dropout, num_layers) )\n",
        "GEN_LMnet = GEN_LM(num_tokens, d, batch_length, num_heads, dropout, num_layers)\n",
        "num_param = number_param(GEN_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# Train network to predict next token\n",
        "optimizer = torch.optim.AdamW(GEN_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "num_epochs = 11 # 101(debug), number of epochs\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_scores = GEN_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%10:\n",
        "        print('Epoch: %d, time(sec): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, time.time()-start, optimizer.param_groups[0]['lr'], loss_epoch) )\n",
        "\n",
        "# generate a new sentence of any length\n",
        "def generate(LMnet, prompt_seq, max_length_gen_seq):\n",
        "    gen_seq = prompt_seq # an initial sequence (a.k.a. prompt) to generate a longer sequence with LMnet\n",
        "    for k in range(max_length_gen_seq):\n",
        "        context_seq = gen_seq[:,-batch_length:] # size=[1, <=batch_length]\n",
        "        score = LMnet(context_seq) # size=[1, batch_length, num_tokens]\n",
        "        score_last_token = score[:,-1,:].squeeze(dim=1) # size=[1, num_tokens]\n",
        "        prob_last_token = torch.softmax(score_last_token, dim=1) # size=[1, num_tokens]\n",
        "        #idx_next_token = torch.multinomial(prob_last_token, num_samples=1) # size=[1,1]\n",
        "        idx_next_token = torch.max(prob_last_token, dim=1).indices[0].view(1,1) # size=[1,1]\n",
        "        gen_seq = torch.cat((gen_seq, idx_next_token), dim=1) # append next token, (size=[1, num_tokens+1]\n",
        "    return gen_seq\n",
        "\n",
        "# generate from a prompt sequence\n",
        "#  prompt_seq = [ 2, 4, 6, 8 ]\n",
        "#     gen_seq =              [ 10, 12, 14, 16, 18, 20, 22 ]\n",
        "prompt_seq = get_batch(seq, batch_size=1, batch_length=4, start_idx=10, list_batch_idx=torch.arange(num_subseq-1))[0] # generate a small sequence to complete, size=[1,batch_length]\n",
        "prompt_tokens = func_int2token(prompt_seq[0].tolist())\n",
        "print('\\nsequence   :', prompt_tokens)\n",
        "gen_seq = generate(GEN_LMnet, prompt_seq, max_length_gen_seq=batch_length)[0][prompt_seq[0].size(0):]\n",
        "seq_tokens = func_int2token(gen_seq.tolist())\n",
        "print('prediction :               ', seq_tokens,'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1BEOirjm5Jr"
      },
      "source": [
        "## Final version of Step #1 : SSL-LLM\n",
        "## Add GPU training, saving pre-trained net, warmup learning rate, stopping condition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cW_IZzbm5Jr",
        "outputId": "37f58e73-403f-411f-ced6-018af607ba95",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n",
            "device: cuda \n",
            "\n",
            "num_tokens: 129, d: 384, batch_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
            "\n",
            "num_net_parameters: 10761345 / 10.76 million\n",
            "\n",
            "checkpoint file : checkpoint/step1_checkpoint_SSL_LM_23-12-05--01-02-01.pkl \n",
            "\n",
            "seq_len: 3000000, batch_size: 500, batch_length: 40, num_subseq: 75000, num_batch: 150\n",
            "\n",
            "num_epochs : 1 \n",
            "\n",
            "Epoch: 0, time(min): 2.543, lr= 0.000300, loss_epoch: 2.160\n",
            "sequence   : <SEP> 5 10 15\n",
            "prediction :              20 25 30 35 40 45 50 55 60 65 70 <SEP> 88 94 100 <SEP> 45 52 59 66 73 80 87 94 <SEP> 36 42 48 54 60 66 72 78 84 90 <SEP> 88 96 <SEP> 92 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# GPU training\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    device = torch.device(\"cuda\") # use GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print('device:',device,'\\n')\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# multiple attention heads layer\n",
        "class multiple_head_attention(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        d_head = d // num_heads\n",
        "        assert d == d_head * num_heads # check divisiblity\n",
        "        self.MHA = nn.MultiheadAttention(d, num_heads, batch_first=True, dropout=dropout)\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length))==0 # mask to make attention to previous tokens only : { token(<=t) }, size=(context_length,context_length)\n",
        "                   # torch.tril(ones) = True in the up-right part, True means *no* attention allowed in pytorch implementation\n",
        "        self.context_length = context_length\n",
        "    def forward(self, H):\n",
        "        if H.size(1) == self.context_length: # training <==\n",
        "            attn_mask = self.mask\n",
        "        else: # when batch_length not= context_length, e.g. inference time / sequence generation <==\n",
        "            current_batch_length = H.size(1)\n",
        "            attn_mask = torch.tril(torch.ones(current_batch_length, current_batch_length))==0\n",
        "        H_heads = self.MHA(H, H, H, attn_mask=attn_mask.to(device))[0] # pytorch implementation, size=[batch_size, batch_length, d]\n",
        "        return H_heads\n",
        "\n",
        "# Transformer block layer\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.MHA = multiple_head_attention(d, context_length, num_heads, dropout)\n",
        "        self.LN_MHA = nn.LayerNorm(d)\n",
        "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n",
        "        self.LN_MLP = nn.LayerNorm(d)\n",
        "    def forward(self, H):\n",
        "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, batch_length, d]\n",
        "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, batch_length, d]\n",
        "        return H\n",
        "\n",
        "# class of self-supervised learning LM network (step 1)\n",
        "class SSL_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length, num_heads, dropout, num_layers):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.transformer_blocks = nn.ModuleList([ TransformerBlock(d, context_length, num_heads, dropout) for _ in range(num_layers) ]) # multiple transformer block layers\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "    def forward(self, batch_seq):\n",
        "        seq_pos_encoding = torch.arange(batch_seq.size(1)).to(device) # positional encoding = {0,1,2,...,batch_length-1}\n",
        "        H = self.token2vec(batch_seq) + self.PE_embedding(seq_pos_encoding).unsqueeze(0) # size=[batch_size, batch_length, d]\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            H = transformer_block(H) # size=[batch_size, batch_length, d]\n",
        "        batch_scores = self.token_prediction(H) # size=[batch_size, batch_length, num_tokens]\n",
        "        return batch_scores # return prediction scores for next token\n",
        "\n",
        "# generate a new sentence of any length\n",
        "def generate(LMnet, prompt_seq, max_length_gen_seq):\n",
        "    gen_seq = prompt_seq # an initial sequence (a.k.a. prompt) to generate a longer sequence with LMnet\n",
        "    for k in range(max_length_gen_seq):\n",
        "        context_seq = gen_seq[:,-batch_length:] # size=[1, <=batch_length]\n",
        "        score = LMnet(context_seq) # size=[1, batch_length, num_tokens]\n",
        "        score_last_token = score[:,-1,:].squeeze(dim=1) # size=[1, num_tokens]\n",
        "        prob_last_token = torch.softmax(score_last_token, dim=1) # size=[1, num_tokens]\n",
        "        #idx_next_token = torch.multinomial(prob_last_token, num_samples=1) # size=[1,1]\n",
        "        idx_next_token = torch.max(prob_last_token, dim=1).indices[0].view(1,1) # size=[1,1]\n",
        "        gen_seq = torch.cat((gen_seq, idx_next_token), dim=1) # append next token, (size=[1, num_tokens+1]\n",
        "    return gen_seq\n",
        "\n",
        "# network parameters\n",
        "d = 128; num_heads = 16; dropout = 0.1; num_layers = 2; batch_length = 20 # bebug\n",
        "d_head = 64; num_heads = 6; d = num_heads * d_head; dropout = 0.1; num_layers = 6; batch_length = 40 # GPU training <==\n",
        "print('num_tokens: %d, d: %d, batch_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, batch_length, num_heads, dropout, num_layers) )\n",
        "SSL_LMnet = SSL_LM(num_tokens, d, batch_length, num_heads, dropout, num_layers)\n",
        "SSL_LMnet = SSL_LMnet.to(device)\n",
        "num_param = number_param(SSL_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(SSL_LMnet.parameters(), lr=3e-4) # standard optimizer for LMs\n",
        "warmup = 1 # 50(debug), 500(GPU), number of batches used for warmup <==\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min(t/warmup, 1.0) ) # warmup learning rate scheduler, good for LM (softmax)\n",
        "\n",
        "# save checkpoint\n",
        "net_parameters = {}\n",
        "net_parameters['num_tokens'] = num_tokens\n",
        "net_parameters['d'] = d\n",
        "net_parameters['num_heads'] = num_heads\n",
        "net_parameters['batch_length'] = batch_length\n",
        "net_parameters['dropout'] = dropout\n",
        "net_parameters['num_layers'] = num_layers\n",
        "checkpoint_dir = os.path.join(\"checkpoint\")\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "print('checkpoint file :', checkpoint_dir + '/step1_checkpoint_SSL_LM_' + time_stamp + '.pkl', '\\n')\n",
        "\n",
        "# batching parameters\n",
        "seq_len = seq.size(0) # length of the long sequence\n",
        "batch_size = 5 # bebug\n",
        "batch_size = 500 # GPU training <==\n",
        "num_subseq = seq_len // batch_length # number of subsequences\n",
        "num_batch = seq_len // (batch_size * batch_length) # number of batches\n",
        "start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # new starting index at each new epoch, random integer in {0,batch_length-1}\n",
        "list_batch_idx = torch.arange(num_batch) # list of batch indices, [0,1,...,num_batch-1]\n",
        "print('seq_len: %d, batch_size: %d, batch_length: %d, num_subseq: %d, num_batch: %d\\n' % (seq_len, batch_size, batch_length, num_subseq, num_batch) )\n",
        "\n",
        "# Train network to predict next token\n",
        "num_epochs = 1 # 1001(debug), 11(GPU), number of epochs <==\n",
        "print('num_epochs :',num_epochs,'\\n')\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_batch_idx = torch.arange(num_subseq-1) # list of batch indices\n",
        "    start_idx = torch.randint(low=0, high=batch_length, size=(1,)) # size=[1]\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for _ in range(num_batch): # number of batches into one epoch\n",
        "        batch_seq, target_seq, list_batch_idx = get_batch(seq, batch_size, batch_length, start_idx, list_batch_idx) # generate a batch of subsequences\n",
        "        batch_seq, target_seq = batch_seq.to(device), target_seq.to(device) # GPU training <==\n",
        "        batch_scores = SSL_LMnet(batch_seq) # size=[batch_size, batch_length, num_tokens]\n",
        "        loss = nn.CrossEntropyLoss()(batch_scores.view(batch_scores.size(0)*batch_length, num_tokens), target_seq.view(batch_scores.size(0)*batch_length)) # classification loss over dict of tokens\n",
        "        running_loss += loss.detach().cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step() # warmup scheduler\n",
        "    loss_epoch = running_loss / num_batch\n",
        "    if not epoch%1: # 100(debug), 1(GPU) <==\n",
        "        print('Epoch: %d, time(min): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, (time.time()-start)/60, optimizer.param_groups[0]['lr'], loss_epoch) )\n",
        "         # save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'tot_time': time.time()-start,\n",
        "            'loss': loss_epoch,\n",
        "            'net_parameters': net_parameters,\n",
        "            'SSL_LMnet_dict': SSL_LMnet.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            }, '{}.pkl'.format(checkpoint_dir + \"/step1_checkpoint_SSL_LM_\" + time_stamp ))\n",
        "        # check prediction performance\n",
        "        prompt_seq = get_batch(seq, batch_size=1, batch_length=4, start_idx=10, list_batch_idx=torch.arange(num_subseq-1))[0].to(device) # generate a small sequence to complete, size=[1,batch_length]\n",
        "        prompt_tokens = func_tokens2str(func_indices2tokens(prompt_seq[0].tolist()))\n",
        "        print('sequence   :', prompt_tokens)\n",
        "        gen_seq = generate(SSL_LMnet, prompt_seq, max_length_gen_seq=batch_length)[0][prompt_seq[0].size(0):]\n",
        "        seq_tokens = func_tokens2str(func_indices2tokens(gen_seq.tolist()))\n",
        "        print('prediction :             ', seq_tokens,'\\n')\n",
        "    # Stopping condition\n",
        "    if loss_epoch < 0.01:\n",
        "        print(\"\\n loss value is small -- training stopped\\n\")\n",
        "        break\n",
        "\n",
        "# GPU training time : Epoch: 10, time(min): 13.235, lr= 0.000300, loss_epoch: 1.031\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjL5Q-pmBfi3"
      },
      "source": [
        "## load pre-trained SSL-LM network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK7z0xkzm5Js",
        "outputId": "2bbc95c2-a14b-4fbb-b852-19009e59c126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load pre-trained SSL-LM: \n",
            " checkpoint file: checkpoint/step1_checkpoint_SSL_LM_23-12-05--01-02-01.pkl\n",
            " epoch: 0, time: 152.599min, loss=2.1597\n",
            " num_tokens: 129, d: 384, batch_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
            "\n",
            "num_net_parameters: 10761345 / 10.76 million\n",
            "\n",
            "sequence   : 12 16 20 24\n",
            "prediction :              28 32 36 40 44 48 52 56 60 64 68 72 <SEP> 95 <SEP> 31 37 43 49 55 61 67 73 79 85 91 97 <SEP> 98 99 100 <SEP> 99 <SEP> 92 97 100 <SEP> 33 40 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "checkpoint_file = checkpoint_dir + '/step1_checkpoint_SSL_LM_' + time_stamp + '.pkl'\n",
        "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "net_parameters = checkpoint['net_parameters']\n",
        "num_tokens = net_parameters['num_tokens']\n",
        "d = net_parameters['d']\n",
        "num_heads = net_parameters['num_heads']\n",
        "batch_length = net_parameters['batch_length']\n",
        "dropout = net_parameters['dropout']\n",
        "num_layers = net_parameters['num_layers']\n",
        "epoch = checkpoint['epoch']\n",
        "tot_time = checkpoint['tot_time']\n",
        "loss = checkpoint['loss']\n",
        "print('Load pre-trained SSL-LM: \\n checkpoint file: {:s}\\n epoch: {:d}, time: {:.3f}min, loss={:.4f}'.format(checkpoint_file,epoch,tot_time,loss))\n",
        "print(' num_tokens: %d, d: %d, batch_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, batch_length, num_heads, dropout, num_layers) )\n",
        "SSL_LMnet = SSL_LM(num_tokens, d, batch_length, num_heads, dropout, num_layers)\n",
        "SSL_LMnet = SSL_LMnet.to(device)\n",
        "SSL_LMnet.load_state_dict(checkpoint['SSL_LMnet_dict'])\n",
        "num_param = number_param(SSL_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "del checkpoint\n",
        "\n",
        "# check pre-trained network : generate from a prompt sequence\n",
        "#  prompt_seq = 2, 4, 6, 8\n",
        "#     gen_seq =             10, 12, 14, 16, 18, 20, 22\n",
        "num_subseq = seq.size(0) // batch_length # number of subsequence\n",
        "prompt_seq = get_batch(seq, batch_size=1, batch_length=4, start_idx=10, list_batch_idx=torch.arange(num_subseq-1))[0].to(device) # generate a small sequence to complete, size=[1,batch_length]\n",
        "prompt_tokens = func_tokens2str(func_indices2tokens(prompt_seq[0].tolist()))\n",
        "print('sequence   :', prompt_tokens)\n",
        "gen_seq = generate(SSL_LMnet, prompt_seq, max_length_gen_seq=batch_length)[0][prompt_seq[0].size(0):]\n",
        "seq_tokens = func_tokens2str(func_indices2tokens(gen_seq.tolist()))\n",
        "print('prediction :             ', seq_tokens,'\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBtADcPlYnVM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRVApgBkYnVM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}