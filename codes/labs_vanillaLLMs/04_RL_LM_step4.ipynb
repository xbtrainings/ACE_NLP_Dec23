{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WculWPI2Cwd8"
      },
      "source": [
        "# Step #4 : Reinforcement Learning (RL) of Language Model (LM)\n",
        "\n",
        "## Task : from prompt, generate response\n",
        "\n",
        "## Use pre-trained models:\n",
        "### SFT-LM model from Step #2 as reference model to fine-tune w.r.t. rank reward\n",
        "### SL-RM model from Step #3 as reward model to evaluate generation\n",
        "\n",
        "## Data structure : prompt\n",
        "\n",
        "### Xavier Bresson, xavier.bresson@gmail.com, https://twitter.com/xbresson\n",
        "\n",
        "### Number of data points for GPT-3, 175B parameters\n",
        "+ Step #1 : 300B tokens\n",
        "+ Step #2 : 10k-100k pairs (prompt, response)\n",
        "+ Step #3 : 100k-1M triples (prompt, positive response, negative response)\n",
        "+ Step #4 : 10k-100k prompts\n",
        "\n",
        "### Number of data points for this tutorial\n",
        "+ Step #1 : 1M tokens\n",
        "+ Step #2 : 10k pairs (prompt, response)\n",
        "+ Step #3 : 10k triples (prompt, positive response, negative response)\n",
        "+ Step #4 : 1k prompts\n",
        "\n",
        "### Objectives\n",
        "+ Adapt PPO reinforcement learning technique to language generation\n",
        "+ Train with batch of prompt training for fast training with GPU\n",
        "+ Use pre-trained models from steps 2 and 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIgfWCW7CweA",
        "outputId": "7df015b0-a665-4560-8502-9f8828ef1189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/ACE_NLP_Dec23_codes/codes/labs_vanillaLLMs\n",
            "/content/gdrive/My Drive/ACE_NLP_Dec23_codes/codes/labs_vanillaLLMs\n"
          ]
        }
      ],
      "source": [
        "# For Google Colaboratory\n",
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    # mount google drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    path_to_file = '/content/gdrive/My Drive/ACE_NLP_Dec23_codes/codes/labs_vanillaLLMs'\n",
        "    print(path_to_file)\n",
        "    # move to Google Drive directory\n",
        "    os.chdir(path_to_file)\n",
        "    !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EPWi0LPuCweC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6705f90d-4ad0-4952-b606-d8aed0b35571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ],
      "source": [
        "# Libraries\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.CRITICAL) # remove warnings\n",
        "import os, datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxsALSBTcEtJ"
      },
      "source": [
        "## Time stamp for save/load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lopymUtRcEtJ",
        "outputId": "0647ac4e-635b-4dc5-a3af-34ff214566b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time_stamp: 23-12-05--01-25-06 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# save time stamp\n",
        "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
        "\n",
        "# check dataset folder exists\n",
        "data_dir = os.path.join('dataset')\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# select a time stamp\n",
        "use_saved_time_stamp = False\n",
        "#use_saved_time_stamp = True\n",
        "if use_saved_time_stamp:\n",
        "    time_stamp = '23-11-28--15-33-57' # trained on GPU on xxx\n",
        "\n",
        "print('time_stamp:', time_stamp, '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzMVhzCkcEtK"
      },
      "source": [
        "## Load dictionary of tokens (step #1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y4hlj8tcEtK",
        "outputId": "1c4ec1fd-6031-442e-c098-67da7a701e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionary: ['18', '23', '28', '33', '38', '43', '<SEP>', '67', '76', '85', '94', '61', '68', '75', '82', '89', '96', '47', '53', '59', '65', '71', '77', '83', '95', '50', '56', '62', '74', '80', '86', '87', '88', '90', '93', '100', '10', '12', '14', '16', '20', '22', '24', '21', '26', '31', '36', '41', '46', '63', '69', '81', '99', '70', '72', '73', '78', '1', '8', '15', '29', '57', '5', '7', '9', '11', '13', '6', '55', '30', '39', '48', '66', '84', '34', '37', '40', '49', '52', '58', '64', '91', '25', '79', '97', '54', '27', '32', '35', '17', '45', '42', '92', '19', '98', '4', '3', '51', '44', '60', '2', '0', 'generate', 'an', 'arithmetic', 'series', 'with', 'terms', 'starting', 'value', 'and', 'common', 'difference', 'Let', 'be', 'the', 'number', 'of', 'then', 'write', 'make', 'a', 'type', 'which', 'starts', 'at', 'elements', '<PAD>', '<EOS>'] \n",
            "\n",
            "num_tokens (unique): 129 \n",
            "\n",
            "token2index: {'18': 0, '23': 1, '28': 2, '33': 3, '38': 4, '43': 5, '<SEP>': 6, '67': 7, '76': 8, '85': 9, '94': 10, '61': 11, '68': 12, '75': 13, '82': 14, '89': 15, '96': 16, '47': 17, '53': 18, '59': 19, '65': 20, '71': 21, '77': 22, '83': 23, '95': 24, '50': 25, '56': 26, '62': 27, '74': 28, '80': 29, '86': 30, '87': 31, '88': 32, '90': 33, '93': 34, '100': 35, '10': 36, '12': 37, '14': 38, '16': 39, '20': 40, '22': 41, '24': 42, '21': 43, '26': 44, '31': 45, '36': 46, '41': 47, '46': 48, '63': 49, '69': 50, '81': 51, '99': 52, '70': 53, '72': 54, '73': 55, '78': 56, '1': 57, '8': 58, '15': 59, '29': 60, '57': 61, '5': 62, '7': 63, '9': 64, '11': 65, '13': 66, '6': 67, '55': 68, '30': 69, '39': 70, '48': 71, '66': 72, '84': 73, '34': 74, '37': 75, '40': 76, '49': 77, '52': 78, '58': 79, '64': 80, '91': 81, '25': 82, '79': 83, '97': 84, '54': 85, '27': 86, '32': 87, '35': 88, '17': 89, '45': 90, '42': 91, '92': 92, '19': 93, '98': 94, '4': 95, '3': 96, '51': 97, '44': 98, '60': 99, '2': 100, '0': 101, 'generate': 102, 'an': 103, 'arithmetic': 104, 'series': 105, 'with': 106, 'terms': 107, 'starting': 108, 'value': 109, 'and': 110, 'common': 111, 'difference': 112, 'Let': 113, 'be': 114, 'the': 115, 'number': 116, 'of': 117, 'then': 118, 'write': 119, 'make': 120, 'a': 121, 'type': 122, 'which': 123, 'starts': 124, 'at': 125, 'elements': 126, '<PAD>': 127, '<EOS>': 128} \n",
            "\n",
            "index2token: {0: '18', 1: '23', 2: '28', 3: '33', 4: '38', 5: '43', 6: '<SEP>', 7: '67', 8: '76', 9: '85', 10: '94', 11: '61', 12: '68', 13: '75', 14: '82', 15: '89', 16: '96', 17: '47', 18: '53', 19: '59', 20: '65', 21: '71', 22: '77', 23: '83', 24: '95', 25: '50', 26: '56', 27: '62', 28: '74', 29: '80', 30: '86', 31: '87', 32: '88', 33: '90', 34: '93', 35: '100', 36: '10', 37: '12', 38: '14', 39: '16', 40: '20', 41: '22', 42: '24', 43: '21', 44: '26', 45: '31', 46: '36', 47: '41', 48: '46', 49: '63', 50: '69', 51: '81', 52: '99', 53: '70', 54: '72', 55: '73', 56: '78', 57: '1', 58: '8', 59: '15', 60: '29', 61: '57', 62: '5', 63: '7', 64: '9', 65: '11', 66: '13', 67: '6', 68: '55', 69: '30', 70: '39', 71: '48', 72: '66', 73: '84', 74: '34', 75: '37', 76: '40', 77: '49', 78: '52', 79: '58', 80: '64', 81: '91', 82: '25', 83: '79', 84: '97', 85: '54', 86: '27', 87: '32', 88: '35', 89: '17', 90: '45', 91: '42', 92: '92', 93: '19', 94: '98', 95: '4', 96: '3', 97: '51', 98: '44', 99: '60', 100: '2', 101: '0', 102: 'generate', 103: 'an', 104: 'arithmetic', 105: 'series', 106: 'with', 107: 'terms', 108: 'starting', 109: 'value', 110: 'and', 111: 'common', 112: 'difference', 113: 'Let', 114: 'be', 115: 'the', 116: 'number', 117: 'of', 118: 'then', 119: 'write', 120: 'make', 121: 'a', 122: 'type', 123: 'which', 124: 'starts', 125: 'at', 126: 'elements', 127: '<PAD>', 128: '<EOS>'} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "load_file_dictionary = 'dataset/step1_02_SSL_dictionary_23-12-05--01-02-01.pt'\n",
        "dictionary, num_tokens, token2index, index2token = torch.load(load_file_dictionary) # load dictionary of tokens\n",
        "print('dictionary:',dictionary,'\\n')\n",
        "print('num_tokens (unique):',num_tokens,'\\n')\n",
        "print('token2index:', token2index,'\\n')\n",
        "print('index2token:', index2token,'\\n')\n",
        "func_tokens2indices = lambda list_tokens: [token2index[token] for token in list_tokens] # ['Let', '5', 'be', 'the'] => [113, 46, 114, 115]\n",
        "func_indices2tokens = lambda list_ints: [index2token[integer] for integer in list_ints] # [113, 46, 114, 115] => ['Let', '5', 'be', 'the']\n",
        "func_str2tokens = lambda input_str: [token_str for token_str in input_str.split()]      # 'Let 5 be the' => ['Let', '5', 'be', 'the']\n",
        "func_tokens2str = lambda list_str: ' '.join(list_str)                                   # ['Let', '5', 'be', 'the'] => 'Let 5 be the'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWH-R0i7cEtK"
      },
      "source": [
        "## Generate the RL training set of prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKAcFLt4cEtK",
        "outputId": "7dd73abd-d905-4e0f-e911-b56613def1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx: 0, time(sec): 0.046\n",
            "number of training data / prompt : 1000 \n",
            "\n",
            "training_set[0] : generate an arithmetic series with 5 terms starting with value 21 and common difference 7  \n",
            "\n",
            "training_set[1] : generate an arithmetic series with 8 terms starting with value 17 and common difference 3  \n",
            "\n",
            "training_set[2] : make a series of arithmetic type which starts at 25 with 13 elements and 5 common difference value  \n",
            "\n",
            "save_file: dataset/step4_01_RL_training_set_23-12-05--01-25-06.pt \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# generate arithmetic series\n",
        "m = max_value = 100 # maximum value in the sequence\n",
        "def arithmetic_series(m, s, d, n):\n",
        "    seq = []\n",
        "    for i in range(n):\n",
        "        v = s + i * d\n",
        "        if v <= m:\n",
        "            seq.append(v)\n",
        "        else:\n",
        "            break\n",
        "    return seq\n",
        "\n",
        "# generate training data, i.e. list of prompts\n",
        "#   prompt = [ generate arithmetic series of 5 terms with difference 2 starting at 3 ]\n",
        "save_training_data = False\n",
        "save_training_data = True\n",
        "if save_training_data:\n",
        "\n",
        "    # \"collect\" training set\n",
        "    list_prompt_RL = []\n",
        "    num_training_data = 12 # debug\n",
        "    num_training_data = 1000 # number of pairs of (prompt, response), e.g. GPU 1,000 pairs (prompt, response)\n",
        "    start = time.time()\n",
        "    for idx in range(num_training_data):\n",
        "\n",
        "        # parameters for arithmetic series\n",
        "        m = max_value # maximum value in the sequence\n",
        "        s = torch.randint(low=0, high=m, size=(1,)).item() # starting integer of the series\n",
        "        d = torch.randint(low=1, high=10, size=(1,)).item() # value of common difference\n",
        "        n = torch.randint(low=5, high=15, size=(1,)).item() # number of element in the series\n",
        "        #print('max_value: %d, start_value: %d, common_difference: %d, number_of_terms: %d' % (m,s,d,n))\n",
        "\n",
        "        # generate prompt : sample a prompt between 3 candidate prompts\n",
        "        prompt = {}\n",
        "        prompt[1] = 'generate an arithmetic series with ' + str(n) + ' terms starting with value ' + str(s) + ' and common difference ' + str(d)\n",
        "        prompt[2] = 'make a series of arithmetic type which starts at ' + str(s) + ' with ' + str(n) + ' elements and ' + str(d) + ' common difference value'\n",
        "        prompt[3] = 'Let ' + str(n) + ' be the number of terms ' + str(s) + ' the starting number and ' + str(d) + ' the common difference then write the arithmetic series'\n",
        "        random_int = torch.randint(low=1, high=3+1, size=(1,)).item() # random number in {1,2,3}\n",
        "        #random_int = 1 # debug\n",
        "        prompt = prompt[random_int]\n",
        "        #response = arithmetic_series(m,s,d,n)\n",
        "\n",
        "        # covert from token to pytorch\n",
        "        prompt = [str(i) for i in prompt.split()] # convert a string into seq of tokens (w/ string type)\n",
        "        prompt = func_tokens2indices(prompt) # convert from token (str) to index (int)\n",
        "        prompt = torch.tensor(prompt) # convert to pytorch\n",
        "\n",
        "        # append\n",
        "        list_prompt_RL.append(prompt)\n",
        "\n",
        "        # track\n",
        "        if not idx%1000:\n",
        "            print('idx: %d, time(sec): %.3f' % (idx, time.time()-start) )\n",
        "\n",
        "    # print\n",
        "    print('number of training data / prompt :',len(list_prompt_RL),'\\n')\n",
        "    for idx, prompt in enumerate(list_prompt_RL[:3]):\n",
        "        prompt = func_tokens2str(func_indices2tokens(prompt.tolist()))\n",
        "        print('training_set[%d] : %s ' % (idx, prompt) , '\\n' )\n",
        "\n",
        "    # save training data\n",
        "    save_file = data_dir + '/step4_01_RL_training_set_' + time_stamp + '.pt'\n",
        "    print('save_file:', save_file, '\\n')\n",
        "    torch.save([list_prompt_RL],save_file) # save list of prompts\n",
        "\n",
        "else:\n",
        "\n",
        "    # load training data\n",
        "    load_file = data_dir + '/step4_01_RL_training_set_' + time_stamp + '.pt'\n",
        "    print('load_file:', load_file, '\\n')\n",
        "    list_prompt_RL = torch.load(load_file)[0]\n",
        "\n",
        "    # print\n",
        "    print('number of training data / prompt :',len(list_prompt_RL),'\\n')\n",
        "    for idx, prompt in enumerate(list_prompt_RL[:3]):\n",
        "        prompt = func_tokens2str(func_indices2tokens(prompt.tolist()))\n",
        "        print('training_set[%d] : %s ' % (idx, prompt) , '\\n' )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDgt2THlcEtL"
      },
      "source": [
        "## Get batch of sampled indices of RL prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Z2mBMvcEtL",
        "outputId": "80649f40-eb7b-4174-c184-2ffb480a6d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_prompt_RL: 1000, batch_size: 100, num_batch_RL: 10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# batching parameters\n",
        "num_prompt_RL = len(list_prompt_RL) # number of prompt sequences\n",
        "batch_size = 3 # debug\n",
        "batch_size = 100 # batch size, 500 GPU\n",
        "num_batch_RL = num_prompt_RL // batch_size # number of batches\n",
        "print('num_prompt_RL: %d, batch_size: %d, num_batch_RL: %d\\n' % (num_prompt_RL,batch_size,num_batch_RL))\n",
        "\n",
        "# sample batch of RL prompt\n",
        "def get_batch_RL(batch_size, list_prompts_idx):\n",
        "    batch_idx = torch.randperm(list_prompts_idx.size(0))[:batch_size] # sample B number of batch indices\n",
        "    batch_idx = list_prompts_idx[batch_idx] # and extract from remaining list of batch indices\n",
        "    if list_prompts_idx.size(0) > batch_size:\n",
        "        new_list_prompts_idx = torch.stack([i for i in list_prompts_idx if i not in batch_idx]) # remove the sampled batch indices from the list of indices\n",
        "    else:\n",
        "        new_list_prompts_idx = torch.tensor([]) # last batch of epoch, i.e. return empty list\n",
        "    return batch_idx, new_list_prompts_idx\n",
        "\n",
        "# # one epoch, debug\n",
        "# list_prompts_idx = torch.arange(num_prompt_RL) # list of RL prompt indices\n",
        "# for i in range(3): # num_batch_RL\n",
        "#     print('batch :',i)\n",
        "#     print('list_prompts_idx (before) :',list_prompts_idx, list_prompts_idx.size())\n",
        "#     batch_idx, list_prompts_idx = get_batch_RL(batch_size, list_prompts_idx) # sample a batch of indices (prompt,response)\n",
        "#     print('batch_idx :', batch_idx, batch_idx.size())\n",
        "#     print('list_prompts_idx (after) :',list_prompts_idx, list_prompts_idx.size(),'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0mP7WiAcEtM"
      },
      "source": [
        "## Transformers backbone for all models, i.e. step #2, step #3 and step #4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOlmTy9IcEtM",
        "outputId": "be4aeb3a-1002-4ae6-ede6-07fedcc2406a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n",
            "device: cuda \n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0) # use same initial seed for reproducibility\n",
        "\n",
        "# compute number of network parameters\n",
        "def number_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    return nb_param\n",
        "\n",
        "# GPU training\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    device = torch.device(\"cuda\") # use GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print('device:',device,'\\n')\n",
        "\n",
        "# token embedding layer : convert seq of integers to seq of vectors\n",
        "class token2vec(nn.Module):\n",
        "    def __init__(self, num_tokens, d):\n",
        "        super().__init__()\n",
        "        self.token2vec = nn.Embedding(num_tokens, d) # map integer to one-hot vector (num_tokens dimensions), and project vector to d-dimentional space\n",
        "    def forward(self, batch_int):\n",
        "        batch_vec = self.token2vec(batch_int) # size=[batch_size, batch_length, d]\n",
        "        return batch_vec\n",
        "\n",
        "# multiple attention heads layer\n",
        "class multiple_head_attention(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        d_head = d // num_heads\n",
        "        assert d == d_head * num_heads # check divisiblity\n",
        "        self.MHA = nn.MultiheadAttention(d, num_heads, batch_first=True, dropout=dropout)\n",
        "        self.mask = torch.tril(torch.ones(context_length, context_length))==0 # mask to make attention to previous tokens only : { token(<=t) }, size=(context_length,context_length)\n",
        "                   # torch.tril(ones) = True in the up-right part, True means *no* attention allowed in pytorch implementation\n",
        "        self.context_length = context_length\n",
        "    def forward(self, H):\n",
        "        if H.size(1) == self.context_length: # training\n",
        "            attn_mask = self.mask\n",
        "        else: # when batch_length not= context_length, e.g. inference time / sequence generation\n",
        "            current_batch_length = H.size(1)\n",
        "            attn_mask = torch.tril(torch.ones(current_batch_length, current_batch_length))==0\n",
        "        H_heads = self.MHA(H, H, H, attn_mask=attn_mask.to(device))[0] # pytorch implementation, size=[batch_size, batch_length, d]\n",
        "        return H_heads\n",
        "\n",
        "# Transformer block layer\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d, context_length, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.MHA = multiple_head_attention(d, context_length, num_heads, dropout)\n",
        "        self.LN_MHA = nn.LayerNorm(d)\n",
        "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n",
        "        self.LN_MLP = nn.LayerNorm(d)\n",
        "    def forward(self, H):\n",
        "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, batch_length, d]\n",
        "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, batch_length, d]\n",
        "        return H\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PJfQIRZcEtM"
      },
      "source": [
        "## Load pre-trained SFT-LM network (step #2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZOLA6fFcEtM",
        "outputId": "9d04a493-f5ca-4dbf-a82d-d49f436bbc09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load pre-trained SFT-LM: \n",
            " checkpoint file: checkpoint/step2_checkpoint_SFT_LM_23-12-05--01-07-33.pkl\n",
            " epoch: 1, time: 452.370min, loss=1.4210\n",
            " num_tokens: 129, d: 384, context_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
            "\n",
            "num_tokens: 129, padding_int: 127, eos_int: 128\n",
            "\n",
            "num_net_parameters: 10761345 / 10.76 million\n",
            "\n",
            "prompt_RL : make a series of arithmetic type which starts at 65 with 14 elements and 1 common difference value\n",
            "gen_seq   : 65 66 67 68 69 70 71 72 73 74 71 74 77 78\n"
          ]
        }
      ],
      "source": [
        "# class of supervised fine-tuning LM network (step 2)\n",
        "class SFT_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length, num_heads, dropout, num_layers, padding_int, eos_int):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.seq_pos_encoding = torch.arange(context_length, device=device) # positional encoding = {0,1,2,...,context_length-1}\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.transformer_blocks = nn.ModuleList([ TransformerBlock(d, context_length, num_heads, dropout) for _ in range(num_layers) ]) # multiple transformer block layers\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "        self.context_length = context_length\n",
        "        self.padding = padding_int\n",
        "        self.eos = eos_int\n",
        "    # Note : No forward function is needed\n",
        "\n",
        "# load pre-trained SFT-LM network (step 2)\n",
        "checkpoint_file = \"checkpoint/step2_checkpoint_SFT_LM_23-12-05--01-07-33.pkl\"\n",
        "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "epoch = checkpoint['epoch']\n",
        "tot_time = checkpoint['tot_time']\n",
        "loss = checkpoint['loss']\n",
        "print('Load pre-trained SFT-LM: \\n checkpoint file: {:s}\\n epoch: {:d}, time: {:.3f}min, loss={:.4f}'.format(checkpoint_file,epoch,tot_time,loss))\n",
        "net_parameters = SFT_LM_net_parameters = checkpoint['net_parameters']\n",
        "num_tokens = net_parameters['num_tokens']\n",
        "d = net_parameters['d']\n",
        "num_heads = net_parameters['num_heads']\n",
        "context_length = net_parameters['context_length']\n",
        "dropout = net_parameters['dropout']\n",
        "num_layers = net_parameters['num_layers']\n",
        "padding_int = net_parameters['padding_int']\n",
        "eos_int = net_parameters['eos_int']\n",
        "print(' num_tokens: %d, d: %d, context_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, context_length, num_heads, dropout, num_layers) )\n",
        "padding_int = torch.tensor([func_tokens2indices('<PAD>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
        "eos_int = torch.tensor([func_tokens2indices('<EOS>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
        "print('num_tokens: %d, padding_int: %d, eos_int: %d\\n' % (num_tokens, padding_int, eos_int))\n",
        "SFT_LMnet = SFT_LM(num_tokens, d, context_length, num_heads, dropout, num_layers, padding_int, eos_int)\n",
        "SFT_LMnet = SFT_LMnet.to(device)\n",
        "SFT_LMnet.load_state_dict(checkpoint['SFT_LMnet_dict']) # load pre-trained SFT-LM network (step 2)\n",
        "num_param = number_param(SFT_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "del checkpoint\n",
        "\n",
        "# check model prediction\n",
        "# generate new sentence of any length\n",
        "def generate(LMnet, prompt, max_length_gen_seq):\n",
        "    LMnet.eval()\n",
        "    predicted_seq = torch.ones(1, max(prompt.size(0),LMnet.context_length)).long().to(device) * LMnet.padding # initiliaze with padding\n",
        "    predicted_seq[:, -prompt.size(0):] = prompt # fill batch_predicted_seq with prompt, right-aligned\n",
        "    for k in range(max_length_gen_seq):\n",
        "        context = predicted_seq[:,-LMnet.context_length:] # size=[batch_size, context_length\n",
        "        H = LMnet.token2vec(context) + LMnet.PE_embedding(LMnet.seq_pos_encoding[:context.size(1)]).unsqueeze(0) # size=[batch_size, context_length, d]\n",
        "        for transformer_block in LMnet.transformer_blocks: H = transformer_block(H) # size=(batch_size, context_length, d)\n",
        "        token_scores = H[:,-1,:] # extract last token to predict the next one, size=[batch_size, d]\n",
        "        token_scores = LMnet.token_prediction(token_scores) # compute scores, size=[batch_size, num_tokens]\n",
        "        token_probs = torch.softmax(token_scores, dim=1) # compute probs, size=[batch_size, num_tokens]\n",
        "        next_token = torch.multinomial(token_probs, num_samples=1) # sample next token, size=[batch_size, 1]\n",
        "        #next_token = torch.max(token_probs, dim=1).indices[0].view(1,1) # size=(1,1)\n",
        "        if next_token==LMnet.eos:\n",
        "            break\n",
        "        predicted_seq = torch.cat((predicted_seq, next_token), dim=1) # size=[batch_size, current_seq_len+1]\n",
        "    gen_seq = predicted_seq[0][max(prompt.size(0),LMnet.context_length):]\n",
        "    return gen_seq\n",
        "prompt_idx = torch.randint(0, num_prompt_RL, (1,))\n",
        "prompt_RL = list_prompt_RL[prompt_idx]\n",
        "print('prompt_RL :',func_tokens2str(func_indices2tokens(prompt_RL.tolist())))\n",
        "gen_seq = generate(SFT_LMnet, prompt_RL, 15)\n",
        "print('gen_seq   :',func_tokens2str(func_indices2tokens(gen_seq.tolist())))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErD7O6oycEtN"
      },
      "source": [
        "## Load pre-trained RM network (step #3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-FrfNlccEtN",
        "outputId": "7fe61da6-8866-4027-f913-fb2f74bcf5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load pre-trained SL-RM: \n",
            " checkpoint file: checkpoint/step3_checkpoint_SL_RM_23-12-05--01-19-09.pkl\n",
            " epoch: 100, time: 61.610min, loss=0.0713\n",
            " num_tokens: 129, d: 384, context_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
            "\n",
            "num_tokens: 129, padding_int: 127, eos_int: 128\n",
            "\n",
            "num_net_parameters: 10910338 / 10.91 million\n",
            "\n",
            "prompt_RL : Let 14 be the number of terms 14 the starting number and 5 the common difference then write the arithmetic series\n",
            "gen_seq   : 14 21 28 23 30 37 38 45 46 53 54 59 68 69\n",
            "rank      : 0.08857535570859909\n"
          ]
        }
      ],
      "source": [
        "# Supervised learning network for reward (step 3)\n",
        "class SL_RM(nn.Module):\n",
        "    def __init__(self, SFT_LM, d, context_length, padding_int, eos_int):\n",
        "        super().__init__()\n",
        "        self.SFT_LM = SFT_LM # token embedding layer\n",
        "        self.reward_prediction = nn.Sequential(nn.LayerNorm(d), nn.Linear(d,d), nn.ReLU(), nn.Linear(d,1)) # reward prediction layer\n",
        "        self.context_length = context_length\n",
        "        self.padding = padding_int\n",
        "        self.eos = eos_int\n",
        "    # Note : No forward function is needed\n",
        "\n",
        "# pre-trained SL-RM network\n",
        "checkpoint_file = 'checkpoint/step3_checkpoint_SL_RM_23-12-05--01-19-09.pkl'\n",
        "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "epoch = checkpoint['epoch']\n",
        "tot_time = checkpoint['tot_time']\n",
        "loss = checkpoint['loss']\n",
        "print('Load pre-trained SL-RM: \\n checkpoint file: {:s}\\n epoch: {:d}, time: {:.3f}min, loss={:.4f}'.format(checkpoint_file,epoch,tot_time,loss))\n",
        "net_parameters = checkpoint['net_parameters']\n",
        "num_tokens = net_parameters['num_tokens']\n",
        "d = net_parameters['d']\n",
        "num_heads = net_parameters['num_heads']\n",
        "context_length = net_parameters['context_length']\n",
        "dropout = net_parameters['dropout']\n",
        "num_layers = net_parameters['num_layers']\n",
        "padding_int = net_parameters['padding_int']\n",
        "eos_int = net_parameters['eos_int']\n",
        "print(' num_tokens: %d, d: %d, context_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, context_length, num_heads, dropout, num_layers) )\n",
        "padding_int = torch.tensor([func_tokens2indices('<PAD>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
        "eos_int = torch.tensor([func_tokens2indices('<EOS>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
        "print('num_tokens: %d, padding_int: %d, eos_int: %d\\n' % (num_tokens, padding_int, eos_int))\n",
        "SL_RMnet = SL_RM(SFT_LMnet, d, context_length, padding_int, eos_int)\n",
        "SL_RMnet = SL_RMnet.to(device)\n",
        "SL_RMnet.load_state_dict(checkpoint['SL_RMnet_dict']) # load pre-trained SL-RM network from step #3\n",
        "num_param = number_param(SL_RMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "del checkpoint\n",
        "\n",
        "# check model prediction\n",
        "# compute rank reward\n",
        "def rank(SL_RMnet, prompt, response):\n",
        "    len_prompt_response = prompt.size(0) + response.size(0)\n",
        "    batch_seq = torch.ones(1, max(len_prompt_response,SL_RMnet.context_length)).long().to(device) * SL_RMnet.padding # initiliaze with padding\n",
        "    batch_seq[:, -len_prompt_response:] = torch.cat((prompt,response),dim=0) # fill batch_predicted_seq with prompt, right-aligned\n",
        "    H = SL_RMnet.SFT_LM.token2vec(batch_seq) + SL_RMnet.SFT_LM.PE_embedding(SL_RMnet.SFT_LM.seq_pos_encoding[:batch_seq.size(1)]).unsqueeze(0) # size=[2* batch_size, context_length, d]\n",
        "    for transformer_block in SL_RMnet.SFT_LM.transformer_blocks: H = transformer_block(H) # size=[1, context_length, d)\n",
        "    token_score = H[:,-1,:] # extract last token scores to predict rewards, size=[1, d]\n",
        "    reward_score = SL_RMnet.reward_prediction(token_score) # compute reward scores, size=[1, 1]\n",
        "    return reward_score\n",
        "prompt_idx = torch.randint(0, num_prompt_RL, (1,))\n",
        "prompt_RL = list_prompt_RL[prompt_idx].to(device)\n",
        "print('prompt_RL :',func_tokens2str(func_indices2tokens(prompt_RL.tolist())))\n",
        "gen_seq = generate(SFT_LMnet, prompt_RL, 15)\n",
        "print('gen_seq   :',func_tokens2str(func_indices2tokens(gen_seq.tolist())))\n",
        "reward_score = rank(SL_RMnet, prompt_RL, gen_seq)\n",
        "print('rank      :',reward_score.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE7JI5aVcEtO"
      },
      "source": [
        "## Reinforcement learning LM network (step #4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85sDVJTRcEtO",
        "outputId": "17e53390-7a65-4469-c5b6-ba54a4618af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters of pre-trained SFT-LM network (step 2)\n",
            " num_tokens: 129, d: 384, context_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
            " num_tokens: 129, padding_int: 127, eos_int: 128\n",
            "\n",
            "num_net_parameters: 10761345 / 10.76 million\n",
            "\n",
            "checkpoint : step4_checkpoint_RL_LM_23-12-05--01-25-06.pkl \n",
            "\n",
            "num_prompt_RL: 1000, batch_size: 25, num_batch_RL: 40\n",
            "\n",
            "num_epochs:  1 \n",
            "\n",
            "Epoch: 0, time(min): 1.200, lr= 0.000010, loss_epoch: -6.122\n",
            "prompt        : generate an arithmetic series with 6 terms starting with value 89 and common difference 1\n",
            "predicted_seq : 89 92 93 92 93 94 <EOS> \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Understanding how to cast the task \"prompt => text generation\" as a reinforcement learning PPO technique\n",
        "#\n",
        "# defined state/s, action/a, reward/r in the LM context :\n",
        "#  state : prompt\n",
        "#  action : generated response with RL-LM network (step 4)\n",
        "#  reward : trained rank score of prompt+response (step 3)\n",
        "#\n",
        "# an episode in the standard RL context :\n",
        "#  s_0 => ... => s_t => a_t ~ policy_net(s_t) => r_t, s_t+1 => ... => end of episode\n",
        "#\n",
        "# an episode in the LM context :\n",
        "#  s = prompt => a = response ~ policy_net(s), r = rank(prompt+response)\n",
        "#\n",
        "# important note : There is NO time t in the RL-LM setting !\n",
        "#                   Mostly because there is no trained reward r_t for partial response\n",
        "#\n",
        "# NO value function V(s) is required to be learned !\n",
        "#  in the standard setting, the value function provides the predicted total discounted reward\n",
        "#                           to reach the end of the episode\n",
        "#  in the LM setting, the value function is given by the learned rank function in step 3\n",
        "#                     it predicts the rank of the prompt+response\n",
        "#  reminder : min_V || V_t - dr_t ||^2, dr_t = sum_{l=0} gamma^l r_t+l\n",
        "#              no t => V = dr = r\n",
        "#\n",
        "# advantage function is simply A = rank in this setting !\n",
        "#  reminder of advantage equation : A_t = sum_{l=0} (gamma * beta)^l delta_t+l, delta_t = r_t + gamma * V_t+1 - V_t\n",
        "#                                   No t => A_t = A = delta = r + (gamma-1) V = r + (gamma-1) r = gamma.r\n",
        "#             min_Policy - min( ratio_t * A_t , clip(ratio_t) * A_t )\n",
        "#         <=> min_Policy - min( ratio * A , clip(ratio) * A )\n",
        "#         <=> min_Policy - min( ratio * gamma.r , clip(ratio) * gamma.r )\n",
        "#         <=> min_Policy - min( ratio * r , clip(ratio) * r ) as gamma>0 does not change the solution\n",
        "#\n",
        "# define the ratio = Policy_Net(a|s) / Policy_Net_previous(a|s), Policy_Net = Probability_RL_LM\n",
        "#                    Policy_Net(response|prompt) / Policy_Net_previous(response|prompt)\n",
        "#\n",
        "# final advantage function is composed of two terms (for maximizing human alignment) :\n",
        "#  advantage = rank(prompt+response) + beta * mean_{token in response} log( Probability_RL_LM(response|prompt) /\n",
        "#                                                                           Probability_SFT_LM(response|prompt) )\n",
        "#              <------------------>           <----------------------------------------------------------------->\n",
        "#             human ranking (step 3)                           human response to prompt (step 2)\n",
        "#\n",
        "# goal : train the RL-LM network to maximize the rank(response)\n",
        "#         but let the network explore diverse responses learned in step 2\n",
        "#         (otherwise, RL-LM will only learn one response)\n",
        "#\n",
        "# class of reinforcement learning LM network (step 4)\n",
        "class RL_LM(nn.Module):\n",
        "    def __init__(self, num_tokens, d, context_length, num_heads, dropout, num_layers, padding_int, eos_int):\n",
        "        super().__init__()\n",
        "        self.token2vec = token2vec(num_tokens, d) # token embedding layer\n",
        "        self.seq_pos_encoding = torch.arange(context_length, device=device) # positional encoding = {0,1,2,...,context_length-1}\n",
        "        self.PE_embedding = nn.Embedding(context_length, d) # positional encoding embedding layer\n",
        "        self.transformer_blocks = nn.ModuleList([ TransformerBlock(d, context_length, num_heads, dropout) for _ in range(num_layers) ]) # multiple transformer block layers\n",
        "        self.token_prediction = nn.Linear(d, num_tokens) # next token prediction layer\n",
        "        self.context_length = context_length\n",
        "        self.padding = padding_int\n",
        "        self.eos = eos_int\n",
        "    # y_RL, prob_PolicyNet_y_RL ~ LM_RL(x_RL)\n",
        "    def forward(self, batch_idx, list_prompt_RL, len_response): # batch_idx.size=[batch_size], len(list_prompt) =[list_prompt_RL]\n",
        "        prompts = [list_prompt_RL[idx] for idx in batch_idx] # sample list of prompts, len(prompts)=num_prompt_response\n",
        "        len_prompt = max([len(prompt) for prompt in prompts]) # compute max of prompt lengths\n",
        "        batch_size = batch_idx.size(0)\n",
        "        y_RL = torch.ones(batch_size, max(len_prompt,self.context_length)).long().to(device) * self.padding # initialize with padding\n",
        "        for idx in range(batch_size): y_RL[idx, -prompts[idx].size(0):] = prompts[idx] # fill batch_predicted_seq with prompt, right-aligned\n",
        "        x_RL = y_RL # batch of RL prompts\n",
        "        prob_PolicyNet_y_RL = torch.tensor([]).to(device)\n",
        "        for idx in range(len_response): # number of auto-regressive prediction\n",
        "            context = y_RL[:,-self.context_length:] # size=[batch_size, context_length\n",
        "            H = self.token2vec(context) + self.PE_embedding(self.seq_pos_encoding[:context.size(1)]).unsqueeze(0) # size=[batch_size, context_length, d]\n",
        "            for transformer_block in self.transformer_blocks: H = transformer_block(H) # size=(batch_size, context_length, d)\n",
        "            token_scores = H[:,-1,:] # extract last token to predict the next one, size=[batch_size, d]\n",
        "            token_scores = self.token_prediction(token_scores) # compute scores, size=[batch_size, num_tokens]\n",
        "            token_probs = torch.softmax(token_scores, dim=1) # compute probs, size=[batch_size, num_tokens]\n",
        "            next_token = torch.multinomial(token_probs, num_samples=1) # sample next token, size=[batch_size, 1]\n",
        "            next_token_probs = token_probs[torch.arange(batch_size), next_token.squeeze()].unsqueeze(1) # probability of next token, size=[batch_size]\n",
        "            y_RL = torch.cat((y_RL, next_token), dim=1) # size=[batch_size, current_seq_len+1]\n",
        "            prob_PolicyNet_y_RL = torch.cat((prob_PolicyNet_y_RL, next_token_probs), dim=1) # size=[batch_size, idx+1]\n",
        "        y_RL = y_RL[:,-len_response:] # size=[batch_size, len_response]\n",
        "        mask_eos = torch.zeros(batch_size, len_response).to(device) # size=[batch_size, len_response]\n",
        "        indices_batch, indices_token = torch.where(y_RL == self.eos) # size=[batch_size, len_(y_RL==self.eos)]\n",
        "        for b in range(batch_size):\n",
        "            indices_all_eos = torch.where(indices_batch == b)[0]\n",
        "            if indices_all_eos.numel()>0:\n",
        "                indices_first_eos = indices_token[indices_all_eos[0]] + 1 # first index s.t. y_RL==self.eos\n",
        "                mask_eos[b,:indices_first_eos] = 1.0 # fill out mask with 1 to identify tokens selected for PPO loss\n",
        "        return x_RL, y_RL, prob_PolicyNet_y_RL, mask_eos # x_RL=[batch_size, context_length], y_RL=[batch_size, len_response], prob_PolicyNet_y_RL=[batch_size, len_response], mask_eos=[batch_size, len_response]\n",
        "    # prob_y_LM_SFT = LM_RL(x_RL+y_RL)\n",
        "    def forward_SFT(self, SFT_LMnet, x_RL, y_RL): # x_RL=[batch_size, context_length], y_RL=[batch_size, len_response]\n",
        "        batch_size = x_RL.size(0); len_response = y_RL.size(1)\n",
        "        xy_RL = torch.cat( (x_RL,y_RL), dim=1) # size=(batch_size, context_length+len_response)\n",
        "        # no need auto-regressive, i.e. one-shot prediction of prob of y_SFT\n",
        "        context = xy_RL[:,-SFT_LMnet.context_length:] # size=[batch_size, context_length]\n",
        "        H = SFT_LMnet.token2vec(context) + SFT_LMnet.PE_embedding(self.seq_pos_encoding[:context.size(1)]).unsqueeze(0) # size=[batch_size, context_length, d]\n",
        "        for transformer_block in SFT_LMnet.transformer_blocks: H = transformer_block(H) # size=(batch_size, context_length, d)\n",
        "        response_token_scores = H[:,-len_response:,:] # extract last token to predict the next one, size=[batch_size, len_response, d]\n",
        "        response_token_scores = SFT_LMnet.token_prediction(response_token_scores) # compute scores, size=[batch_size, len_response, num_tokens]\n",
        "        response_token_probs = torch.softmax(response_token_scores, dim=2) # compute probs, size=[batch_size, len_response, num_tokens]\n",
        "        prob_y_LM_SFT = torch.tensor([]).to(device)\n",
        "        for idx in range(len_response):\n",
        "            prob_y_t = response_token_probs[torch.arange(batch_size),idx,y_RL[torch.arange(batch_size),idx]].unsqueeze(1) # size=[batch_size, 1]\n",
        "            prob_y_LM_SFT = torch.cat((prob_y_LM_SFT,prob_y_t),dim=1) # size=[batch_size, current_seq_len+1]\n",
        "        return prob_y_LM_SFT # size=[batch_size, len_response]\n",
        "    # RM_xy_RL = RM(x_RL+y_RL)\n",
        "    def forward_RM(self, SL_RMnet, x_RL, y_RL):  # x_RL=[batch_size, context_length], y_RL=[batch_size, len_response]\n",
        "        batch_size = x_RL.size(0); len_response = y_RL.size(1)\n",
        "        xy_RL = torch.cat( (x_RL,y_RL), dim=1) # size=(batch_size, context_length+len_response)\n",
        "        # no need auto-regressive, i.e. one-shot prediction of prob of RM(x_RL+y_RL)\n",
        "        context = xy_RL[:,-SL_RMnet.context_length:] # size=[batch_size, context_length]\n",
        "        H = SL_RMnet.SFT_LM.token2vec(context) + SL_RMnet.SFT_LM.PE_embedding(self.seq_pos_encoding[:context.size(1)]).unsqueeze(0) # size=[batch_size, context_length, d]\n",
        "        for transformer_block in SL_RMnet.SFT_LM.transformer_blocks: H = transformer_block(H) # size=(batch_size, context_length, d)\n",
        "        token_scores = H[:,-1,:] # extract last token to predict rewards, size=[batch_size, len_response, d]\n",
        "        RM_xy_RL = SL_RMnet.reward_prediction(token_scores).squeeze() # compute reward scores, size=[batch_size]\n",
        "        return RM_xy_RL # size=[batch_size]\n",
        "\n",
        "\n",
        "# use parameters of pre-trained SFT-LM network (step 2) for RL_LM network\n",
        "print('Parameters of pre-trained SFT-LM network (step 2)')\n",
        "num_tokens = net_parameters['num_tokens']\n",
        "d = net_parameters['d']\n",
        "num_heads = net_parameters['num_heads']\n",
        "context_length = net_parameters['context_length']\n",
        "dropout = net_parameters['dropout']\n",
        "num_layers = net_parameters['num_layers']\n",
        "padding_int = net_parameters['padding_int']\n",
        "eos_int = net_parameters['eos_int']\n",
        "print(' num_tokens: %d, d: %d, context_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d' % (num_tokens, d, context_length, num_heads, dropout, num_layers) )\n",
        "padding_int = torch.tensor([func_tokens2indices('<PAD>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
        "eos_int = torch.tensor([func_tokens2indices('<EOS>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
        "print(' num_tokens: %d, padding_int: %d, eos_int: %d\\n' % (num_tokens, padding_int, eos_int))\n",
        "\n",
        "# RL_LM network\n",
        "RL_LMnet = RL_LM(num_tokens, d, context_length, num_heads, dropout, num_layers, padding_int, eos_int)\n",
        "RL_LMnet = RL_LMnet.to(device)\n",
        "num_param = number_param(RL_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "\n",
        "# initialize RL-LM with pre-trained SFT-LM network (step 2)\n",
        "checkpoint_file = \"checkpoint/step2_checkpoint_SFT_LM_23-12-05--01-07-33.pkl\"\n",
        "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "RL_LMnet.load_state_dict(checkpoint['SFT_LMnet_dict'])\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(RL_LMnet.parameters(), lr=1e-5) # lr must be smaller because RM can have high value\n",
        "warmup = 1 # 50(debug), 50(GPU), number of batches used for warmup <==\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min(t/warmup, 1.0) ) # warmup learning rate scheduler, good for LM (softmax)\n",
        "clip_value = 0.2 # clipping value for PPO\n",
        "num_iter_policy_loss = 4 # 10 number of iteration for policy loss\n",
        "beta = 0.01 # weight for similarity between RL policy (i.e. human preferences) and SFT-LM (i.e. human prompt-response)\n",
        "\n",
        "# save checkpoint\n",
        "net_parameters = {}\n",
        "net_parameters['num_tokens'] = num_tokens\n",
        "net_parameters['d'] = d\n",
        "net_parameters['num_heads'] = num_heads\n",
        "net_parameters['context_length'] = context_length\n",
        "net_parameters['dropout'] = dropout\n",
        "net_parameters['num_layers'] = num_layers\n",
        "net_parameters['padding_int'] = padding_int\n",
        "net_parameters['eos_int'] = eos_int\n",
        "print('checkpoint :',\"step4_checkpoint_RL_LM_\" + time_stamp + '.pkl', '\\n')\n",
        "checkpoint_dir = os.path.join(\"checkpoint\")\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# batching parameters\n",
        "num_prompt_RL = len(list_prompt_RL) # number of prompt sequences\n",
        "batch_size = 3 # debug\n",
        "batch_size = 50 # batch size, 500 GPU <==\n",
        "batch_size = 25\n",
        "num_batch_RL = num_prompt_RL // batch_size # number of batches\n",
        "print('num_prompt_RL: %d, batch_size: %d, num_batch_RL: %d\\n' % (num_prompt_RL, batch_size, num_batch_RL))\n",
        "\n",
        "# Train network to predict response from prompt\n",
        "len_response = 15\n",
        "num_epochs = 1 # 1001(debug), 11(GPU), number of epochs <==\n",
        "print('num_epochs: ',num_epochs,'\\n')\n",
        "start = time.time()\n",
        "for epoch in range(num_epochs): # number of epochs\n",
        "    list_prompts_RL_idx = torch.arange(num_prompt_RL).to(device) # initialize the list of prompt\n",
        "    running_loss = 0.0 # tracking total loss value\n",
        "    for k in range(num_batch_RL): # number of batches in one epoch\n",
        "        # y_RL, prob_PolicyNet_y_RL ~ LM_RL(x_RL) : get a batch x_RL of RL prompts and generate responses y_RL and their probabilities prob_PolicyNet_y_RL\n",
        "        batch_idx, list_prompts_RL_idx = get_batch_RL(batch_size, list_prompts_RL_idx) # sample a batch of indices (prompt,response)\n",
        "        x_RL, y_RL, prob_PolicyNet_y_RL, mask_eos = RL_LMnet(batch_idx.to(device), list_prompt_RL, len_response) # x_RL=[batch_size, context_length], y_RL=[batch_size, len_response], prob_PolicyNet_y_RL=[batch_size, len_response]\n",
        "        # RM(x_RL+y_RL) : compute rank score of RL responses with SL-RM network\n",
        "        RM_xy_RL = RL_LMnet.forward_RM(SL_RMnet, x_RL, y_RL) # size=[batch_size]\n",
        "        # prob_y_LM_SFT = LM_RL(x_RL+y_RL) : compute probabilities prob_y_SFT of RL responses with reference LM-SFT network\n",
        "        prob_y_LM_SFT = RL_LMnet.forward_SFT(SFT_LMnet, x_RL, y_RL) # size=[batch_size, len_response]\n",
        "        # compute advantage function for PPO loss\n",
        "        advantage = RM_xy_RL - beta * ( torch.log(prob_PolicyNet_y_RL) - torch.log(prob_y_LM_SFT) ).mean(dim=1) # 0.1 # size=[batch_size]\n",
        "        advantage = advantage.unsqueeze(1).detach() # size=[batch_size,1]\n",
        "        # Run PPO a few iterations\n",
        "        log_probs_previous = torch.log(prob_PolicyNet_y_RL) # use log_probs from generation step as reference (fixed during optimization), size=[batch_size, len_response]\n",
        "        for k in range(num_iter_policy_loss):\n",
        "            _, _, prob_PolicyNet_y_RL, _ = RL_LMnet(batch_idx.to(device), list_prompt_RL, len_response) # from same prompts, generate new probabilities, size=[batch_size, len_response]\n",
        "            log_probs = torch.log(prob_PolicyNet_y_RL) # size=[batch_size, len_response]\n",
        "            policy_ratio = torch.exp( log_probs - log_probs_previous.detach()) # ratio between new optimized policy and previous one, size=[batch_size, len_response]\n",
        "            clipped_ratio = policy_ratio.clamp(1.0 - clip_value, 1.0 + clip_value) # clipped ratio to allow small changes only, size=[batch_size, len_response]\n",
        "            policy_ratio = mask_eos * policy_ratio   # tokens after eos do not contribute to the loss, size=[batch_size, len_response]\n",
        "            clipped_ratio = mask_eos * clipped_ratio # tokens after eos do not contribute to the loss, size=[batch_size, len_response]\n",
        "            loss = - torch.min( policy_ratio * advantage , clipped_ratio * advantage ).mean() # select the loss with smallest change, scalar\n",
        "            running_loss += loss.detach().cpu().item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "    loss_epoch = running_loss / num_batch_RL\n",
        "    if not epoch%1: # 10(debug), 1(GPU) <==\n",
        "        print('Epoch: %d, time(min): %.3f, lr= %.6f, loss_epoch: %.3f' % (epoch, (time.time()-start)/60, optimizer.param_groups[0]['lr'], loss_epoch) )\n",
        "        # save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'tot_time': time.time()-start,\n",
        "            'loss': loss_epoch,\n",
        "            'net_parameters': net_parameters,\n",
        "            'RL_LMnet_dict': RL_LMnet.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            }, '{}.pkl'.format(checkpoint_dir + \"/step4_checkpoint_RL_LM_\" + time_stamp ))\n",
        "        # print one prompt\n",
        "        idx_prompt = 0\n",
        "        print('prompt        :',func_tokens2str(func_indices2tokens(x_RL[idx_prompt][torch.where(x_RL[idx_prompt]==padding_int)[0][-1]+1:].tolist())))   # remove all padding tokens\n",
        "        print('predicted_seq :',func_tokens2str(func_indices2tokens(y_RL[idx_prompt][:torch.where(y_RL[idx_prompt]==eos_int)[0][0]+1].tolist())),'\\n' ) # remove all tokens from first eos token\n",
        "#         # Stopping condition\n",
        "#         if loss_epoch < 0.1:\n",
        "#             print(\"\\n loss value is small -- training stopped\\n\")\n",
        "#             break\n",
        "\n",
        "# GPU training time : Epoch: 4, time(min): 2.185, lr= 0.000010, loss_epoch: -24.283\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8KWrZ-HcEtP"
      },
      "source": [
        "## Load pre-trained RL-LM network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psVHHGY8cEtP",
        "outputId": "6d8da6d0-9c78-4b36-f9a7-353d00f02bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load pre-trained RL-LM: \n",
            " checkpoint file: checkpoint/step4_checkpoint_RL_LM_23-12-05--01-25-06.pkl\n",
            " epoch: 0, time: 72.016min, loss=-6.1217\n",
            " num_tokens: 129, d: 384, context_length: 40, num_heads: 6, dropout: 0.10, num_layers: 6\n",
            "\n",
            "num_tokens: 129, padding_int: 127, eos_int: 128\n",
            "\n",
            "num_net_parameters: 10761345 / 10.76 million\n",
            "\n",
            "idx_prompt : 76\n",
            "prompt     : generate an arithmetic series with 11 terms starting with value 10 and common difference 8\n",
            "gen_seq    : 10 16 26 34 34 52 58 66 74 86 90\n"
          ]
        }
      ],
      "source": [
        "# pre-trained SL-RM network\n",
        "checkpoint_file = checkpoint_dir + '/step4_checkpoint_RL_LM_' + time_stamp + '.pkl'\n",
        "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "epoch = checkpoint['epoch']\n",
        "tot_time = checkpoint['tot_time']\n",
        "loss = checkpoint['loss']\n",
        "print('Load pre-trained RL-LM: \\n checkpoint file: {:s}\\n epoch: {:d}, time: {:.3f}min, loss={:.4f}'.format(checkpoint_file,epoch,tot_time,loss))\n",
        "net_parameters = checkpoint['net_parameters']\n",
        "num_tokens = net_parameters['num_tokens']\n",
        "d = net_parameters['d']\n",
        "num_heads = net_parameters['num_heads']\n",
        "context_length = net_parameters['context_length']\n",
        "dropout = net_parameters['dropout']\n",
        "num_layers = net_parameters['num_layers']\n",
        "padding_int = net_parameters['padding_int']\n",
        "eos_int = net_parameters['eos_int']\n",
        "print(' num_tokens: %d, d: %d, context_length: %d, num_heads: %d, dropout: %.2f, num_layers: %d\\n' % (num_tokens, d, context_length, num_heads, dropout, num_layers) )\n",
        "padding_int = torch.tensor([func_tokens2indices('<PAD>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
        "eos_int = torch.tensor([func_tokens2indices('<EOS>'.split())[0]]).to(device) # end-of-sentence token for batch\n",
        "print('num_tokens: %d, padding_int: %d, eos_int: %d\\n' % (num_tokens, padding_int, eos_int))\n",
        "RL_LMnet = RL_LM(num_tokens, d, context_length, num_heads, dropout, num_layers, padding_int, eos_int)\n",
        "RL_LMnet = RL_LMnet.to(device)\n",
        "RL_LMnet.load_state_dict(checkpoint['RL_LMnet_dict']) # load pre-trained RL-LM network from step #4\n",
        "num_param = number_param(RL_LMnet)\n",
        "print('num_net_parameters: %d / %.2f million\\n' % (num_param, num_param/1e6) )\n",
        "del checkpoint\n",
        "\n",
        "# generate new sentence of any length\n",
        "def generate(LMnet, prompt, max_length_gen_seq):\n",
        "    LMnet.eval()\n",
        "    predicted_seq = torch.ones(1, max(prompt.size(0),LMnet.context_length)).long().to(device) * LMnet.padding # initiliaze with padding\n",
        "    predicted_seq[:, -prompt.size(0):] = prompt # fill batch_predicted_seq with prompt, right-aligned\n",
        "    for k in range(max_length_gen_seq):\n",
        "        context = predicted_seq[:,-LMnet.context_length:] # size=[batch_size, context_length\n",
        "        H = LMnet.token2vec(context) + LMnet.PE_embedding(LMnet.seq_pos_encoding[:context.size(1)]).unsqueeze(0) # size=[batch_size, context_length, d]\n",
        "        for transformer_block in LMnet.transformer_blocks: H = transformer_block(H) # size=(batch_size, context_length, d)\n",
        "        token_scores = H[:,-1,:] # extract last token to predict the next one, size=[batch_size, d]\n",
        "        token_scores = LMnet.token_prediction(token_scores) # compute scores, size=[batch_size, num_tokens]\n",
        "        token_probs = torch.softmax(token_scores, dim=1) # compute probs, size=[batch_size, num_tokens]\n",
        "        next_token = torch.multinomial(token_probs, num_samples=1) # sample next token, size=[batch_size, 1]\n",
        "        #next_token = torch.max(token_probs, dim=1).indices[0].view(1,1) # size=(1,1)\n",
        "        if next_token==LMnet.eos:\n",
        "            break\n",
        "        predicted_seq = torch.cat((predicted_seq, next_token), dim=1) # size=[batch_size, current_seq_len+1]\n",
        "    gen_seq = predicted_seq[0][max(prompt.size(0),LMnet.context_length):]\n",
        "    return gen_seq\n",
        "\n",
        "# print one prompt\n",
        "idx_prompt = torch.randint(low=0, high=num_prompt_RL, size=(1,)).item() # random number in {0,...,num_prompt_RL-1}\n",
        "print('idx_prompt :',idx_prompt)\n",
        "prompt = list_prompt_RL[idx_prompt]\n",
        "print('prompt     :',func_tokens2str(func_indices2tokens(prompt.tolist())))\n",
        "gen_seq = generate(RL_LMnet, prompt, max_length_gen_seq=15)\n",
        "print('gen_seq    :',func_tokens2str(func_indices2tokens(gen_seq.tolist())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRGVIYjgcEtP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUvE2fRhcEtQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}